{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#template-de-entrega-atv1","title":"Template de Entrega atv1","text":"Edi\u00e7\u00e3o <p>2025.2</p>"},{"location":"#grupo","title":"Grupo","text":"<ol> <li>Vinicius Grecco</li> </ol>"},{"location":"#exercicios-data-science","title":"Exerc\u00edcios - Data Science","text":""},{"location":"#atv1","title":"ATV1","text":"<ul> <li>Exerc\u00edcio 1</li> <li>Exerc\u00edcio 2 - Neural Networks </li> <li>Exerc\u00edcio 3</li> </ul>"},{"location":"#atv2","title":"ATV2","text":"<ul> <li>Exerc\u00edcios \u2013 Perceptron</li> </ul>"},{"location":"#atv3","title":"ATV3","text":"<ul> <li>Exerc\u00edcios - MLP</li> </ul>"},{"location":"#atv4-vae","title":"ATV4 - VAE","text":"<ul> <li>Exerc\u00edcio - VAE</li> </ul>"},{"location":"#teste","title":"teste","text":"<p>Instru\u00e7\u00f5es</p> <p>Voc\u00eas devem utilizar este template como um bloco de notas para registrar o que foi feito e o que falta fazer. Voc\u00eas devem adicionar as informa\u00e7\u00f5es necess\u00e1rias. O template deve ser editado e atualizado a cada entrega, registrando assim a data de entrega e o que foi feito at\u00e9 o momento via Git.</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#diagramas","title":"Diagramas","text":"<p>Use o Mermaid para criar os diagramas de documenta\u00e7\u00e3o.</p> <p>Mermaid Live Editor</p> <pre><code>flowchart TD\n    Deployment:::orange --&gt;|defines| ReplicaSet\n    ReplicaSet --&gt;|manages| pod((Pod))\n    pod:::red --&gt;|runs| Container\n    Deployment --&gt;|scales| pod\n    Deployment --&gt;|updates| pod\n\n    Service:::orange --&gt;|exposes| pod\n\n    subgraph  \n        ConfigMap:::orange\n        Secret:::orange\n    end\n\n    ConfigMap --&gt; Deployment\n    Secret --&gt; Deployment\n    classDef red fill:#f55\n    classDef orange fill:#ffa500</code></pre>"},{"location":"#codigos","title":"C\u00f3digos","text":"De um arquivo remotoAnota\u00e7\u00f5es no c\u00f3digo main.yaml<pre><code>name: ci\non:\n  - push\n  - pull_request\n\n# Environment\nenv:\n  CI: true\n  PYTHON_VERSION: 3.12\n\n# Jobs to run\njobs:\n\n  # Build and deploy documentation site\n  deploy:\n    if: github.event_name != 'pull_request' &amp;&amp; github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n\n      # Checkout source form GitHub\n      - uses: actions/checkout@v4\n\n      # Install Python runtime and dependencies\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n\n      # pip\n      - run: |\n          pip install -r requirements.txt\n\n      # deploy\n      - run: |\n          mkdocs gh-deploy --force\n</code></pre> compose.yaml<pre><code>name: app\n\n    db:\n        image: postgres:17\n        environment:\n            POSTGRES_DB: ${POSTGRES_DB:-projeto} # (1)!\n            POSTGRES_USER: ${POSTGRES_USER:-projeto}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-projeto}\n        ports:\n            - 5432:5432 #(2)!\n</code></pre> <ol> <li> <p>Caso a vari\u00e1vel de ambiente <code>POSTGRES_DB</code> n\u00e3o exista ou seja nula - n\u00e3o seja definida no arquivo <code>.env</code> - o valor padr\u00e3o ser\u00e1 <code>projeto</code>. Vide documenta\u00e7\u00e3o.</p> </li> <li> <p>Aqui \u00e9 feito um t\u00fanel da porta 5432 do container do banco de dados para a porta 5432 do host (no caso localhost). Em um ambiente de produ\u00e7\u00e3o, essa porta n\u00e3o deve ser exposta, pois ningu\u00e9m de fora do compose deveria acessar o banco de dados diretamente.</p> </li> </ol>"},{"location":"#exemplo-de-video","title":"Exemplo de v\u00eddeo","text":"<p>Lorem ipsum dolor sit amet</p>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"#exercicios-data-science_1","title":"Exerc\u00edcios - Data Science","text":"<ul> <li>Exerc\u00edcio 1</li> <li>Exerc\u00edcio 2 - Neural Networks </li> <li>Exerc\u00edcio 3</li> </ul>"},{"location":"1-Data/ex01/","title":"Neural Networks - Ex01","text":""},{"location":"1-Data/ex01/#artificial-neural-networks-and-deep-learning","title":"Artificial Neural Networks and Deep Learning","text":""},{"location":"1-Data/ex01/#ex01-exploring-class-separability-in-2d","title":"Ex01 - Exploring Class Separability in 2D","text":"<pre><code># Imports necess\u00e1rios\nimport numpy as np\nimport matplotlib.pyplot as plt\n</code></pre> <pre><code># Gerando dataset\nx0 = np.random.normal(2, 0.8, 100)    \ny0 = np.random.normal(3, 2.5, 100)\n\nx1 = np.random.normal(5, 1.2, 100)    \ny1 = np.random.normal(6, 1.9, 100)\n\nx2 = np.random.normal(8, 0.9, 100)    \ny2 = np.random.normal(1, 0.9, 100)\n\nx3 = np.random.normal(15, 0.5, 100)    \ny3 = np.random.normal(4, 2.0, 100)\n\n# Manter arrays separados para plotar\nclasses = [\n    np.column_stack([x0, y0]),\n    np.column_stack([x1, y1]),\n    np.column_stack([x2, y2]),\n    np.column_stack([x3, y3])\n]\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib.colors import ListedColormap\n\n# Assumindo que voc\u00ea j\u00e1 tem os dados x0,y0,x1,y1,x2,y2,x3,y3 definidos\n# Se n\u00e3o tiver, descomente as linhas abaixo para dados de exemplo:\n\n\n# Preparar dados para os algoritmos\nX = np.vstack([\n    np.column_stack([x0, y0]),\n    np.column_stack([x1, y1]),\n    np.column_stack([x2, y2]),\n    np.column_stack([x3, y3])\n])\n\ny = np.hstack([\n    np.zeros(len(x0)),\n    np.ones(len(x1)),\n    np.full(len(x2), 2),\n    np.full(len(x3), 3)\n])\n\n# Fun\u00e7\u00e3o para plotar fronteiras de decis\u00e3o\ndef plot_decision_boundary(clf, X, y, title, ax):\n    h = 0.1\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                        np.arange(y_min, y_max, h))\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    colors = ['lightblue', 'lightcoral', 'lightgreen', 'lightgray']\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    ax.contourf(xx, yy, Z, alpha=0.3, cmap=cmap)\n    ax.contour(xx, yy, Z, colors='black', linestyles='--', linewidths=1)\n\n# Criar figura com subplots\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\naxes = axes.flatten()\n\n# 1. M\u00e9todo Manual (o seu c\u00f3digo original)\nax = axes[0]\nax.scatter(x0, y0, color='blue', label=\"classe0\")\nax.scatter(x1, y1, color='red', label=\"classe1\")\nax.scatter(x2, y2, color='green', label=\"classe2\")\nax.scatter(x3, y3, color='black', label=\"classe3\")\n\n# Suas linhas manuais\nax.plot([2.5, 4.8], [10, 0], 'k--', linewidth=2)\nax.plot([3.8, 10], [-1.0, 7.8], 'k--', linewidth=2)\nax.plot([12.0, 12.0], [-3, 10], 'k--', linewidth=2)\n\nax.legend()\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_title('1. Fronteiras Manuais')\n\n# 2. SVM com kernel linear\nax = axes[1]\nsvm_linear = SVC(kernel='linear', C=1.0)\nsvm_linear.fit(X, y)\nplot_decision_boundary(svm_linear, X, y, 'SVM Linear', ax)\n\nax.scatter(x0, y0, color='blue', label=\"classe0\", s=50, edgecolors='black')\nax.scatter(x1, y1, color='red', label=\"classe1\", s=50, edgecolors='black')\nax.scatter(x2, y2, color='green', label=\"classe2\", s=50, edgecolors='black')\nax.scatter(x3, y3, color='black', label=\"classe3\", s=50, edgecolors='white')\nax.legend()\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_title('2. SVM Linear')\n\n# 3. SVM com kernel RBF\nax = axes[2]\nsvm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')\nsvm_rbf.fit(X, y)\nplot_decision_boundary(svm_rbf, X, y, 'SVM RBF', ax)\n\nax.scatter(x0, y0, color='blue', label=\"classe0\", s=50, edgecolors='black')\nax.scatter(x1, y1, color='red', label=\"classe1\", s=50, edgecolors='black')\nax.scatter(x2, y2, color='green', label=\"classe2\", s=50, edgecolors='black')\nax.scatter(x3, y3, color='black', label=\"classe3\", s=50, edgecolors='white')\nax.legend()\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_title('3. SVM RBF')\n\n# 4. Decision Tree\nax = axes[3]\ndt = DecisionTreeClassifier(max_depth=5, random_state=42)\ndt.fit(X, y)\nplot_decision_boundary(dt, X, y, 'Decision Tree', ax)\n\nax.scatter(x0, y0, color='blue', label=\"classe0\", s=50, edgecolors='black')\nax.scatter(x1, y1, color='red', label=\"classe1\", s=50, edgecolors='black')\nax.scatter(x2, y2, color='green', label=\"classe2\", s=50, edgecolors='black')\nax.scatter(x3, y3, color='black', label=\"classe3\", s=50, edgecolors='white')\nax.legend()\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_title('4. \u00c1rvore de Decis\u00e3o')\n\n# 5. K-Nearest Neighbors\nax = axes[4]\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X, y)\nplot_decision_boundary(knn, X, y, 'KNN', ax)\n\nax.scatter(x0, y0, color='blue', label=\"classe0\", s=50, edgecolors='black')\nax.scatter(x1, y1, color='red', label=\"classe1\", s=50, edgecolors='black')\nax.scatter(x2, y2, color='green', label=\"classe2\", s=50, edgecolors='black')\nax.scatter(x3, y3, color='black', label=\"classe3\", s=50, edgecolors='white')\nax.legend()\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_title('5. K-Nearest Neighbors')\n\n# 6. Random Forest\nax = axes[5]\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X, y)\nplot_decision_boundary(rf, X, y, 'Random Forest', ax)\n\nax.scatter(x0, y0, color='blue', label=\"classe0\", s=50, edgecolors='black')\nax.scatter(x1, y1, color='red', label=\"classe1\", s=50, edgecolors='black')\nax.scatter(x2, y2, color='green', label=\"classe2\", s=50, edgecolors='black')\nax.scatter(x3, y3, color='black', label=\"classe3\", s=50, edgecolors='white')\nax.legend()\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_title('6. Random Forest')\n\nplt.tight_layout()\nplt.show()\n\n# Opcional: Imprimir acur\u00e1cias dos modelos\nmodels = {\n    'SVM Linear': svm_linear,\n    'SVM RBF': svm_rbf,\n    'Decision Tree': dt,\n    'KNN': knn,\n    'Random Forest': rf\n}\n\nprint(\"\\nAcur\u00e1cias dos modelos:\")\nfor name, model in models.items():\n    accuracy = model.score(X, y)\n    print(f\"{name}: {accuracy:.3f}\")\n</code></pre> <pre><code>Acur\u00e1cias dos modelos:\nSVM Linear: 0.973\nSVM RBF: 0.973\nDecision Tree: 0.988\nKNN: 0.973\nRandom Forest: 1.000\n</code></pre>"},{"location":"1-Data/ex01/#exercicios-data-science","title":"Exerc\u00edcios - Data Science","text":"<ul> <li>Exerc\u00edcio 1</li> <li>Exerc\u00edcio 2 - Neural Networks </li> <li>Exerc\u00edcio 3</li> </ul>"},{"location":"1-Data/ex02/","title":"Neural Networks - Ex02","text":""},{"location":"1-Data/ex02/#artificial-neural-networks-and-deep","title":"Artificial Neural Networks and Deep","text":""},{"location":"1-Data/ex02/#ex02-non-linearity-in-higher-dimensions","title":"Ex02 - Non-Linearity in Higher Dimensions","text":"<pre><code>import numpy as np\n</code></pre> <pre><code># Parametros da funcao:\nmeanA = [0,0,0,0,0]\ncovA = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0]\n])\n\nmeanB = [1.5, 1.5, 1.5, 1.5, 1.5]\ncovB = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5]\n])\n\ndataA = np.random.multivariate_normal(meanA,covA,500)\ndataB = np.random.multivariate_normal(meanB,covB,500)\n</code></pre> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nX = np.vstack([dataA, dataB])\ny = np.array(['A']*len(dataA) + ['B']*len(dataB))\n\nX_std = StandardScaler().fit_transform(X)\n\npca = PCA(n_components=2)\nZ = pca.fit_transform(X_std)\n</code></pre> <pre><code># ---- a partir do seu c\u00f3digo (dataA, dataB j\u00e1 criados) ----\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# 1) empilha as duas classes e cria os r\u00f3tulos\nX = np.vstack([dataA, dataB])            # shape (1000, 5)\ny = np.array(['A']*len(dataA) + ['B']*len(dataB))\n\n# 2) padroniza (m\u00e9dia 0, desvio 1) antes do PCA\nX_std = StandardScaler().fit_transform(X)\n\n# 3) PCA para 2 componentes\npca = PCA(n_components=2)\nZ = pca.fit_transform(X_std)             # shape (1000, 2)\n\n# opcional: quanta vari\u00e2ncia PC1+PC2 explicam?\nprint(\"Explained variance ratio (PC1, PC2):\", pca.explained_variance_ratio_)\nprint(\"Total 2D variance explained:\", pca.explained_variance_ratio_.sum())\n\n# 4) scatter 2D colorindo por classe\nplt.figure(figsize=(6,4))\nfor lab, col in zip(('A','B'), ('tab:blue','tab:orange')):\n    idx = (y == lab)\n    plt.scatter(Z[idx, 0], Z[idx, 1], s=18, label=f'Class {lab}')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <pre><code>Explained variance ratio (PC1, PC2): [0.52017441 0.15376346]\nTotal 2D variance explained: 0.6739378707782059\n</code></pre> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.manifold import Isomap, LocallyLinearEmbedding, SpectralEmbedding, TSNE\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import silhouette_score\n\ntry:\n    from sklearn.manifold import trustworthiness\n    HAS_TRUST = True\nexcept Exception:\n    HAS_TRUST = False\n    def trustworthiness(*args, **kwargs):\n        return np.nan\n\nX = np.vstack([dataA, dataB])                 # (1000, 5)\ny = np.array(['A']*len(dataA) + ['B']*len(dataB))  # (1000,)\n\n# Padroniza\u00e7\u00e3o (m\u00e9dia 0, desvio 1)\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X)\n\n# -----------------------------\n# 2) Definir m\u00e9todos (6 no total)\n# -----------------------------\nmethods = {}\n\n# 1) PCA (baseline linear)\nmethods['PCA'] = PCA(n_components=2)\n\n# 2) Kernel PCA (RBF) para capturar n\u00e3o-linearidade\nmethods['KernelPCA (RBF)'] = KernelPCA(\n    n_components=2, kernel='rbf', gamma=0.5, fit_inverse_transform=False\n)\n\n# 3) Isomap (dist\u00e2ncias geod\u00e9sicas no grafo de vizinhos)\nmethods['Isomap'] = Isomap(n_components=2, n_neighbors=15)\n\n# 4) LLE (Local Linear Embedding)\nmethods['LLE'] = LocallyLinearEmbedding(\n    n_neighbors=15, n_components=2, method='standard'\n)\n\n# 5) Spectral Embedding (Laplacian Eigenmaps)\nmethods['Spectral Embedding'] = SpectralEmbedding(\n    n_components=2, n_neighbors=15, random_state=42\n)\n\n# 6) t-SNE (boa separa\u00e7\u00e3o local para visualiza\u00e7\u00e3o)\nmethods['t-SNE'] = TSNE(\n    n_components=2, perplexity=30, learning_rate='auto', init='pca', random_state=42\n)\n\n# -----------------------------\n# 3) Fun\u00e7\u00f5es auxiliares\n# -----------------------------\ndef run_embedding(model, X_input):\n    \"\"\"Aplica fit_transform quando existir; sen\u00e3o fit + transform.\"\"\"\n    if hasattr(model, 'fit_transform'):\n        Z = model.fit_transform(X_input)\n    else:\n        model.fit(X_input)\n        Z = model.transform(X_input)\n    return Z\n\ndef plot_embedding(Z, title, y):\n    \"\"\"Scatter 2D com as duas classes (sem definir cores explicitamente).\"\"\"\n    plt.figure(figsize=(6, 4))\n    idxA = (y == 'A')\n    idxB = ~idxA\n    plt.scatter(Z[idxA, 0], Z[idxA, 1], s=12, label='Class A')\n    plt.scatter(Z[idxB, 0], Z[idxB, 1], s=12, label='Class B')\n    plt.title(title)\n    plt.xlabel('Component 1')\n    plt.ylabel('Component 2')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\ndef fisher_separation(Z, y):\n    \"\"\"\n    Medida simples tipo Fisher para 2 classes:\n    ||muA - muB||^2 / trace(Sw),\n    onde Sw = cov(ZA) + cov(ZB)\n    \"\"\"\n    ZA = Z[y == 'A']\n    ZB = Z[y == 'B']\n    muA = ZA.mean(axis=0)\n    muB = ZB.mean(axis=0)\n    Sw = np.cov(ZA, rowvar=False) + np.cov(ZB, rowvar=False)\n    num = np.sum((muA - muB)**2)\n    den = np.trace(Sw)\n    return float(num / den) if den &gt; 0 else np.nan\n\ndef knn_cv_accuracy(Z, y, n_splits=5, n_neighbors=5):\n    \"\"\"Acur\u00e1cia m\u00e9dia 5-fold CV de um k-NN (k=5) no embedding 2D.\"\"\"\n    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n    scores = cross_val_score(clf, Z, y, cv=cv, scoring='accuracy')\n    return float(scores.mean())\n\n# -----------------------------\n# 4) Rodar m\u00e9todos + coletar m\u00e9tricas\n# -----------------------------\nrows = []\n\nif not HAS_TRUST:\n    print(\"[aviso] sklearn.manifold.trustworthiness n\u00e3o dispon\u00edvel nesta vers\u00e3o; m\u00e9tricas 'Trustworthiness' ser\u00e3o NaN.\\n\")\n\nfor name, model in methods.items():\n    Z = run_embedding(model, X_std)\n\n    # Plota (figura separada por m\u00e9todo)\n    plot_embedding(Z, f'{name}', y)\n\n    # M\u00e9tricas\n    tw = trustworthiness(X_std, Z, n_neighbors=12)\n    sil = silhouette_score(Z, y)        # separa\u00e7\u00e3o das classes no 2D\n    acc = knn_cv_accuracy(Z, y, n_splits=5, n_neighbors=5)\n    fish = fisher_separation(Z, y)\n\n    # PCA: vari\u00e2ncia explicada em 2D\n    if name == 'PCA':\n        pca_var2d = float(np.sum(methods['PCA'].explained_variance_ratio_))\n    else:\n        pca_var2d = np.nan\n\n    rows.append({\n        'Method': name,\n        'Trustworthiness@12': tw,\n        'Silhouette': sil,\n        'kNN(5) CV Acc': acc,\n        'Fisher Ratio': fish,\n        'Explained Var (2D)': pca_var2d\n    })\n\n# -----------------------------\n# 5) Mostrar tabela de m\u00e9tricas\n# -----------------------------\ndf_metrics = pd.DataFrame(rows)\ndf_metrics = df_metrics.sort_values(\n    by=['kNN(5) CV Acc', 'Silhouette', 'Trustworthiness@12'],\n    ascending=False\n).reset_index(drop=True)\n\nprint(\"\\n=== Dimensionality Reduction: Comparison Metrics ===\")\nprint(df_metrics.to_string(index=False))\n</code></pre> <pre><code>=== Dimensionality Reduction: Comparison Metrics ===\n            Method  Trustworthiness@12  Silhouette  kNN(5) CV Acc  Fisher Ratio  Explained Var (2D)\n             t-SNE            0.977410    0.461134          0.920      2.826775                 NaN\nSpectral Embedding            0.851930    0.352003          0.902      1.062955                 NaN\n   KernelPCA (RBF)            0.787827    0.325156          0.892      1.043269                 NaN\n               LLE            0.856462    0.274128          0.875      0.839438                 NaN\n               PCA            0.864511    0.362713          0.863      1.696896            0.673938\n            Isomap            0.868698    0.382801          0.858      1.876618                 NaN\n</code></pre>"},{"location":"1-Data/ex02/#relatorio-de-analise-tecnicas-de-reducao-de-dimensionalidade","title":"Relat\u00f3rio de An\u00e1lise: T\u00e9cnicas de Redu\u00e7\u00e3o de Dimensionalidade","text":""},{"location":"1-Data/ex02/#1-analise-das-relacoes-entre-classes","title":"1. An\u00e1lise das Rela\u00e7\u00f5es entre Classes","text":""},{"location":"1-Data/ex02/#11-principal-component-analysis-pca","title":"1.1 Principal Component Analysis (PCA)","text":"<p>A an\u00e1lise via PCA linear revela que as duas classes se distinguem principalmente ao longo do primeiro componente principal (PC1). Observa-se uma sobreposi\u00e7\u00e3o consider\u00e1vel na regi\u00e3o central dos dados, onde as distribui\u00e7\u00f5es el\u00edpticas dos dois grupos se interpenetram. Este comportamento indica a presen\u00e7a de um deslocamento de m\u00e9dia entre as classes A e B, conforme esperado dado que \u00b5_B = 1.5 em todas as dimens\u00f5es. No entanto, as diferentes matrizes de covari\u00e2ncia resultam em orienta\u00e7\u00f5es e formatos elipsoidais distintos, causando mistura dos pontos quando projetados no espa\u00e7o bidimensional.</p>"},{"location":"1-Data/ex02/#12-kernel-pca-rbf","title":"1.2 Kernel PCA (RBF)","text":"<p>A aplica\u00e7\u00e3o do Kernel PCA com fun\u00e7\u00e3o de base radial (RBF) revela uma fronteira curva com formato caracter\u00edstico de \"banana\" entre os grupos. Esta t\u00e9cnica demonstra sua capacidade de capturar estruturas n\u00e3o-lineares que n\u00e3o s\u00e3o detect\u00e1veis pelo PCA linear convencional. Embora a separa\u00e7\u00e3o apresente melhoria em rela\u00e7\u00e3o ao PCA linear, ainda persiste uma zona de transi\u00e7\u00e3o entre as classes.</p>"},{"location":"1-Data/ex02/#13-isomap","title":"1.3 Isomap","text":"<p>A proje\u00e7\u00e3o via Isomap mant\u00e9m similaridades com o PCA linear, apresentando classes separadas ao longo de um eixo principal, por\u00e9m com distor\u00e7\u00f5es que resultam no alongamento da nuvem de pontos. A t\u00e9cnica preserva a faixa de sobreposi\u00e7\u00e3o caracter\u00edstica, confirmando que um \u00fanico hiperplano n\u00e3o \u00e9 suficiente para separa\u00e7\u00e3o perfeita dos dados.</p>"},{"location":"1-Data/ex02/#14-locally-linear-embedding-lle","title":"1.4 Locally Linear Embedding (LLE)","text":"<p>O LLE apresenta resultados consistentes com o PCA linear, mantendo a tend\u00eancia de separa\u00e7\u00e3o linear com mistura na regi\u00e3o central. Este comportamento sugere que a estrutura local dos dados n\u00e3o oferece informa\u00e7\u00f5es adicionais para uma divis\u00e3o mais eficiente sem a introdu\u00e7\u00e3o de curvatura adicional.</p>"},{"location":"1-Data/ex02/#15-spectral-embedding","title":"1.5 Spectral Embedding","text":"<p>A t\u00e9cnica de Spectral Embedding produz uma estrutura em formato de arco (horseshoe), onde as classes ocupam diferentes metades do arco com bordas que se entremeiam. Esta visualiza\u00e7\u00e3o evidencia claramente a natureza n\u00e3o-linear da fronteira de separa\u00e7\u00e3o \u00f3tima.</p>"},{"location":"1-Data/ex02/#16-t-distributed-stochastic-neighbor-embedding-t-sne","title":"1.6 t-distributed Stochastic Neighbor Embedding (t-SNE)","text":"<p>O t-SNE gera dois aglomerados bem definidos e separados no plano bidimensional, com m\u00ednima sobreposi\u00e7\u00e3o visual. \u00c9 importante considerar que o t-SNE prioriza a preserva\u00e7\u00e3o de vizinhan\u00e7as locais em detrimento de dist\u00e2ncias e formas globais. Portanto, embora a visualiza\u00e7\u00e3o sugira separa\u00e7\u00e3o clara, isso n\u00e3o garante a exist\u00eancia de um corte linear no espa\u00e7o original pentadimensional.</p>"},{"location":"1-Data/ex02/#2-separabilidade-linear-das-classes","title":"2. Separabilidade Linear das Classes","text":""},{"location":"1-Data/ex02/#21-analise-de-separabilidade","title":"2.1 An\u00e1lise de Separabilidade","text":"<p>Com base nas evid\u00eancias obtidas atrav\u00e9s das diferentes t\u00e9cnicas de redu\u00e7\u00e3o de dimensionalidade, conclui-se que as classes n\u00e3o s\u00e3o perfeitamente separ\u00e1veis de forma linear. Os m\u00e9todos lineares (PCA, LLE) demonstram consistentemente a presen\u00e7a de uma faixa de interpenetra\u00e7\u00e3o entre as classes. Mesmo quando t\u00e9cnicas como t-SNE sugerem separa\u00e7\u00e3o visual, isso n\u00e3o implica necessariamente a exist\u00eancia de um hiperplano separador no espa\u00e7o original.</p>"},{"location":"1-Data/ex02/#22-fundamentacao-teorica","title":"2.2 Fundamenta\u00e7\u00e3o Te\u00f3rica","text":"<p>A impossibilidade de separa\u00e7\u00e3o linear perfeita decorre das diferen\u00e7as entre as matrizes de covari\u00e2ncia das classes (\u03a3_A \u2260 \u03a3_B). Quando duas distribui\u00e7\u00f5es gaussianas possuem matrizes de covari\u00e2ncia id\u00eanticas, a fronteira \u00f3tima baseada na raz\u00e3o de verossimilhan\u00e7a \u00e9 linear (an\u00e1logo ao Linear Discriminant Analysis). Entretanto, quando as covari\u00e2ncias diferem, a fronteira Bayesiana \u00f3tima torna-se quadr\u00e1tica, caracterizando um problema de Quadratic Discriminant Analysis (QDA).</p> <p>No presente estudo, a classe A apresenta correla\u00e7\u00f5es espec\u00edficas (0.8 entre x\u2081 e x\u2082, 0.5 entre x\u2083 e x\u2084), enquanto a classe B possui correla\u00e7\u00f5es distintas (-0.7 entre x\u2081 e x\u2082, 0.6 entre x\u2083 e x\u2084). Essas diferen\u00e7as nas orienta\u00e7\u00f5es e alongamentos el\u00edpticos resultam no cruzamento das nuvens de pontos, impossibilitando a separa\u00e7\u00e3o sem erro atrav\u00e9s de um hiperplano.</p>"},{"location":"1-Data/ex02/#3-limitacoes-de-modelos-lineares-e-beneficios-de-nao-linearidades","title":"3. Limita\u00e7\u00f5es de Modelos Lineares e Benef\u00edcios de N\u00e3o-linearidades","text":""},{"location":"1-Data/ex02/#31-limitacoes-dos-modelos-lineares","title":"3.1 Limita\u00e7\u00f5es dos Modelos Lineares","text":"<p>Modelos lineares como Perceptron e Regress\u00e3o Log\u00edstica (sem features n\u00e3o-lineares) implementam exclusivamente fronteiras de decis\u00e3o planares. Dado que as classes no presente estudo requerem fronteiras curvas (quadr\u00e1ticas ou de maior complexidade) para minimizar a sobreposi\u00e7\u00e3o, um hiperplano necessariamente intersectar\u00e1 a zona de mistura, resultando em erros de classifica\u00e7\u00e3o significativos.</p>"},{"location":"1-Data/ex02/#32-vantagens-dos-modelos-nao-lineares","title":"3.2 Vantagens dos Modelos N\u00e3o-lineares","text":"<p>Modelos com capacidade de n\u00e3o-linearidade, incluindo redes neurais multi-camada com fun\u00e7\u00f5es de ativa\u00e7\u00e3o ReLU/tanh, m\u00e9todos kernel (SVM com kernel RBF), e QDA, possuem a habilidade de \"curvar\" o espa\u00e7o de caracter\u00edsticas ou compor fun\u00e7\u00f5es complexas para aproximar a fronteira Bayesiana \u00f3tima.</p> <p>As evid\u00eancias visuais obtidas atrav\u00e9s de Kernel PCA, Spectral Embedding e t-SNE corroboram a presen\u00e7a de separa\u00e7\u00e3o curva e estruturas tipo manifold, indicando que m\u00faltiplas regi\u00f5es devem ser delimitadas por fun\u00e7\u00f5es n\u00e3o-lineares para alcan\u00e7ar classifica\u00e7\u00e3o eficaz entre as classes A e B.</p>"},{"location":"1-Data/ex02/#4-conclusoes-e-recomendacoes","title":"4. Conclus\u00f5es e Recomenda\u00e7\u00f5es","text":"<p>As an\u00e1lises de redu\u00e7\u00e3o de dimensionalidade revelam que as classes A e B apresentam diferencia\u00e7\u00e3o prim\u00e1ria ao longo do primeiro componente principal, por\u00e9m com sobreposi\u00e7\u00e3o substancial nas regi\u00f5es centrais. M\u00e9todos n\u00e3o-lineares evidenciam fronteiras curvas e melhor separa\u00e7\u00e3o local, caracter\u00edsticas n\u00e3o detect\u00e1veis atrav\u00e9s de proje\u00e7\u00e3o linear.</p> <p>Considerando que \u03a3_A \u2260 \u03a3_B, a fronteira \u00f3tima entre as distribui\u00e7\u00f5es gaussianas \u00e9 intrinsecamente quadr\u00e1tica, portanto n\u00e3o-linear. Consequentemente, um hiperplano n\u00e3o pode proporcionar separa\u00e7\u00e3o adequada dos dados.</p> <p>Recomenda-se, portanto, a utiliza\u00e7\u00e3o de modelos com capacidade n\u00e3o-linear, tais como redes neurais multi-camada com fun\u00e7\u00f5es de ativa\u00e7\u00e3o ReLU/tanh, SVM com kernel RBF, ou an\u00e1lise discriminante quadr\u00e1tica (QDA), para capturar adequadamente a geometria curva caracter\u00edstica do problema de classifica\u00e7\u00e3o em quest\u00e3o.</p>"},{"location":"1-Data/ex02/#anexos-exercicios-praticos","title":"Anexos - Exerc\u00edcios Pr\u00e1ticos","text":"<ul> <li>Exerc\u00edcio 1: An\u00e1lise Explorat\u00f3ria de Dados</li> <li>Exerc\u00edcio 2: Redes Neurais </li> <li>Exerc\u00edcio 3: Modelagem Avan\u00e7ada</li> </ul>"},{"location":"1-Data/ex03_again/","title":"Neural Networks - Ex03","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\n</code></pre> <pre><code>df = pd.read_csv(\"data/spaceship-titanic/train.csv\")\ndf.head(5)\n</code></pre> PassengerId HomePlanet CryoSleep Cabin Destination Age VIP RoomService FoodCourt ShoppingMall Spa VRDeck Name Transported 0 0001_01 Europa False B/0/P TRAPPIST-1e 39.0 False 0.0 0.0 0.0 0.0 0.0 Maham Ofracculy False 1 0002_01 Earth False F/0/S TRAPPIST-1e 24.0 False 109.0 9.0 25.0 549.0 44.0 Juanna Vines True 2 0003_01 Europa False A/0/S TRAPPIST-1e 58.0 True 43.0 3576.0 0.0 6715.0 49.0 Altark Susent False 3 0003_02 Europa False A/0/S TRAPPIST-1e 33.0 False 0.0 1283.0 371.0 3329.0 193.0 Solam Susent False 4 0004_01 Earth False F/1/S TRAPPIST-1e 16.0 False 303.0 70.0 151.0 565.0 2.0 Willy Santantines True <pre><code>print(df.columns)\n</code></pre> <pre><code>Index(['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age',\n       'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',\n       'Name', 'Transported'],\n      dtype='object')\n</code></pre> <pre><code># Apaga linhas com mais de 20% dos valores nulos\ndf = df[df.isnull().mean(axis=1) &lt;= 0.2]\n</code></pre>"},{"location":"1-Data/ex03_again/#explorando-valores-das-colunas","title":"Explorando valores das colunas","text":"<pre><code>num_cols = df.select_dtypes(include=[np.number]).columns\ncat_cols = df.select_dtypes(exclude=[np.number]).columns\n</code></pre> <pre><code>for col in num_cols:\n    print(f\"Coluna: {col}\")\n    print(df[col].describe())  # estat\u00edsticas b\u00e1sicas\n    print(f\"Valores nulos: {df[col].isnull().sum()}\")\n\n    # Compara\u00e7\u00e3o de medidas centrais\n    mean_val = df[col].mean()\n    median_val = df[col].median()\n    mode_val = df[col].mode()[0] if not df[col].mode().empty else None\n\n    print(f\"M\u00e9dia  : {mean_val}\")\n    print(f\"Mediana: {median_val}\")\n    print(f\"Moda   : {mode_val}\")\n\n    print(\"=========================================================\")\n    print(\"=========================================================\")\n</code></pre> <pre><code>Coluna: Age\ncount    8502.000000\nmean       28.833686\nstd        14.488328\nmin         0.000000\n25%        19.000000\n50%        27.000000\n75%        38.000000\nmax        79.000000\nName: Age, dtype: float64\nValores nulos: 174\nM\u00e9dia  : 28.833686191484357\nMediana: 27.0\nModa   : 24.0\n=========================================================\n=========================================================\nColuna: RoomService\ncount     8495.000000\nmean       224.907946\nstd        667.277226\nmin          0.000000\n25%          0.000000\n50%          0.000000\n75%         47.000000\nmax      14327.000000\nName: RoomService, dtype: float64\nValores nulos: 181\nM\u00e9dia  : 224.90794585050028\nMediana: 0.0\nModa   : 0.0\n=========================================================\n=========================================================\nColuna: FoodCourt\ncount     8496.000000\nmean       458.294845\nstd       1612.265649\nmin          0.000000\n25%          0.000000\n50%          0.000000\n75%         76.250000\nmax      29813.000000\nName: FoodCourt, dtype: float64\nValores nulos: 180\nM\u00e9dia  : 458.2948446327684\nMediana: 0.0\nModa   : 0.0\n=========================================================\n=========================================================\nColuna: ShoppingMall\ncount     8472.000000\nmean       173.909821\nstd        605.124872\nmin          0.000000\n25%          0.000000\n50%          0.000000\n75%         27.000000\nmax      23492.000000\nName: ShoppingMall, dtype: float64\nValores nulos: 204\nM\u00e9dia  : 173.909820585458\nMediana: 0.0\nModa   : 0.0\n=========================================================\n=========================================================\nColuna: Spa\ncount     8498.000000\nmean       311.479407\nstd       1137.450945\nmin          0.000000\n25%          0.000000\n50%          0.000000\n75%         59.000000\nmax      22408.000000\nName: Spa, dtype: float64\nValores nulos: 178\nM\u00e9dia  : 311.4794069192751\nMediana: 0.0\nModa   : 0.0\n=========================================================\n=========================================================\nColuna: VRDeck\ncount     8491.000000\nmean       304.638323\nstd       1146.250510\nmin          0.000000\n25%          0.000000\n50%          0.000000\n75%         45.500000\nmax      24133.000000\nName: VRDeck, dtype: float64\nValores nulos: 185\nM\u00e9dia  : 304.63832293016134\nMediana: 0.0\nModa   : 0.0\n=========================================================\n=========================================================\n</code></pre> <pre><code>for col in cat_cols:\n    print(f\"Coluna: {col}\")\n    print(f\"N\u00famero de valores distintos: {df[col].nunique()}\")\n    print(f\"Valores nulos: {df[col].isnull().sum()}\")\n    print(df[col].value_counts())\n    print(\"=========================================================\")\n    print(\"=========================================================\")\n</code></pre> <pre><code>Coluna: PassengerId\nN\u00famero de valores distintos: 8676\nValores nulos: 0\nPassengerId\n0001_01    1\n6133_01    1\n6139_05    1\n6139_04    1\n6139_03    1\n          ..\n3124_01    1\n3123_01    1\n3121_01    1\n3120_02    1\n9280_02    1\nName: count, Length: 8676, dtype: int64\n=========================================================\n=========================================================\nColuna: HomePlanet\nN\u00famero de valores distintos: 3\nValores nulos: 196\nHomePlanet\nEarth     4595\nEuropa    2129\nMars      1756\nName: count, dtype: int64\n=========================================================\n=========================================================\nColuna: CryoSleep\nN\u00famero de valores distintos: 2\nValores nulos: 211\nCryoSleep\nFalse    5434\nTrue     3031\nName: count, dtype: int64\n=========================================================\n=========================================================\nColuna: Cabin\nN\u00famero de valores distintos: 6552\nValores nulos: 194\nCabin\nG/734/S     8\nG/1368/P    7\nF/1194/P    7\nB/201/P     7\nB/11/S      7\n           ..\nE/231/S     1\nG/545/S     1\nG/543/S     1\nB/106/P     1\nC/178/S     1\nName: count, Length: 6552, dtype: int64\n=========================================================\n=========================================================\nColuna: Destination\nN\u00famero de valores distintos: 3\nValores nulos: 176\nDestination\nTRAPPIST-1e      5906\n55 Cancri e      1800\nPSO J318.5-22     794\nName: count, dtype: int64\n=========================================================\n=========================================================\nColuna: VIP\nN\u00famero de valores distintos: 2\nValores nulos: 199\nVIP\nFalse    8278\nTrue      199\nName: count, dtype: int64\n=========================================================\n=========================================================\nColuna: Name\nN\u00famero de valores distintos: 8461\nValores nulos: 195\nName\nJuane Popelazquez     2\nSus Coolez            2\nAnkalik Nateansive    2\nGwendy Sykess         2\nKeitha Josey          2\n                     ..\nHardy Griffy          1\nSalley Mckinn         1\nMall Frasp            1\nHeila Gordond         1\nPropsh Hontichre      1\nName: count, Length: 8461, dtype: int64\n=========================================================\n=========================================================\nColuna: Transported\nN\u00famero de valores distintos: 2\nValores nulos: 0\nTransported\nTrue     4368\nFalse    4308\nName: count, dtype: int64\n=========================================================\n=========================================================\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# ===============================\n# 0) C\u00f3pia e remo\u00e7\u00f5es b\u00e1sicas\n# ===============================\ndfp = df.copy()\n\n# alvo (0/1) e remo\u00e7\u00e3o de colunas n\u00e3o preditivas\ny = dfp['Transported'].map({True: 1, False: 0}).astype('int8')\ndfp = dfp.drop(columns=['Transported', 'PassengerId', 'Name'])\n\n# ===============================\n# 1) Quebrar Cabin em Deck / Num / Side\n# ===============================\ndef split_cabin(s):\n    if pd.isna(s):\n        return pd.Series([np.nan, np.nan, np.nan], index=['Deck','CabinNum','Side'])\n    parts = str(s).split('/')\n    # garante 3 partes\n    parts += [np.nan] * (3 - len(parts))\n    return pd.Series(parts[:3], index=['Deck','CabinNum','Side'])\n\ndfp[['Deck','CabinNum','Side']] = dfp['Cabin'].apply(split_cabin)\ndfp.drop(columns=['Cabin'], inplace=True)\ndfp['CabinNum'] = pd.to_numeric(dfp['CabinNum'], errors='coerce')\n\n# ===============================\n# 2) Definir grupos de colunas\n# ===============================\nnum_cols_base = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'CabinNum']\nbin_cols      = ['CryoSleep', 'VIP']\ncat_cols      = ['HomePlanet', 'Destination', 'Deck', 'Side']\n\n# ===============================\n# 3) Imputa\u00e7\u00e3o\n# ===============================\n# 3.1 Num\u00e9ricas -&gt; mediana\nfor c in num_cols_base:\n    dfp[c] = dfp[c].fillna(dfp[c].median())\n\n# 3.2 Bin\u00e1rias -&gt; moda e para 0/1\nfor c in bin_cols:\n    if dfp[c].isnull().any():\n        dfp[c] = dfp[c].fillna(dfp[c].mode()[0])\n    dfp[c] = dfp[c].astype(bool).astype('int8')\n\n# 3.3 Categ\u00f3ricas -&gt; moda\nfor c in cat_cols:\n    if dfp[c].isnull().any():\n        dfp[c] = dfp[c].fillna(dfp[c].mode()[0])\n\n# ===============================\n# 4) Redu\u00e7\u00e3o de assimetria nas despesas (opcional, recomendado)\n# ===============================\nspend_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\nfor c in spend_cols:\n    # log1p: lida bem com zeros (log(0+1)=0)\n    dfp[c] = np.log1p(dfp[c])\n\n# ===============================\n# 5) One-Hot nas categ\u00f3ricas de baixa cardinalidade\n# ===============================\ndfp = pd.get_dummies(dfp, columns=cat_cols, drop_first=False, dtype='int8')\n\n# ===============================\n# 6) Escalonamento para tanh\n#    Escolha 1: StandardScaler (z-score)\n# ===============================\nscaler = StandardScaler()\n\n# ATEN\u00c7\u00c3O: em modelagem real, use scaler.fit(X_train[num_cols]) e depois transforme\nnum_cols_final = ['Age', 'CabinNum'] + spend_cols\ndfp[num_cols_final] = scaler.fit_transform(dfp[num_cols_final])\n\n# (Se preferir MinMax para [-1,1], troque as duas linhas acima por:)\n# scaler = MinMaxScaler(feature_range=(-1, 1))\n# dfp[num_cols_final] = scaler.fit_transform(dfp[num_cols_final])\n\n# ===============================\n# 7) Conjunto final\n# ===============================\nX = dfp  # todas as features num\u00e9ricas j\u00e1 prontas\n# y j\u00e1 est\u00e1 definido como 0/1\nprint(X.shape, y.shape)\nprint(\"Nulos restantes em X?\", X.isnull().sum().sum())\n</code></pre> <pre><code>(8676, 25) (8676,)\nNulos restantes em X? 0\n\n\n/var/folders/x9/_d9131zd0kv_7kqrvjfvld9m0000gn/T/ipykernel_34466/3708549578.py:46: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  dfp[c] = dfp[c].fillna(dfp[c].mode()[0])\n/var/folders/x9/_d9131zd0kv_7kqrvjfvld9m0000gn/T/ipykernel_34466/3708549578.py:46: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  dfp[c] = dfp[c].fillna(dfp[c].mode()[0])\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n\n# ---- Age: antes (bruto)\ndf['Age'].hist(bins=30)\nplt.title(\"Age - antes do escalonamento\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Freq\")\nplt.show()\n\n# ---- Age: depois (z-score)\nX['Age'].hist(bins=30)\nplt.title(\"Age - depois do escalonamento (z-score)\")\nplt.xlabel(\"Age (padronizado)\")\nplt.ylabel(\"Freq\")\nplt.show()\n\n# ---- FoodCourt: antes (bruto, altamente assim\u00e9trico)\ndf['FoodCourt'].hist(bins=50)\nplt.title(\"FoodCourt - antes (bruto, assim\u00e9trico)\")\nplt.xlabel(\"FoodCourt\")\nplt.ylabel(\"Freq\")\nplt.show()\n\n# ---- FoodCourt: ap\u00f3s log1p + z-score\nX['FoodCourt'].hist(bins=50)\nplt.title(\"FoodCourt - ap\u00f3s log1p + z-score\")\nplt.xlabel(\"FoodCourt (transformado)\")\nplt.ylabel(\"Freq\")\nplt.show()\n</code></pre> <pre><code>X.head(10)\n</code></pre> CryoSleep Age VIP RoomService FoodCourt ShoppingMall Spa VRDeck CabinNum HomePlanet_Earth ... Deck_A Deck_B Deck_C Deck_D Deck_E Deck_F Deck_G Deck_T Side_P Side_S 0 0 0.711326 0 -0.638230 -0.650345 -0.623215 -0.664376 -0.639889 -1.176597 0 ... 0 1 0 0 0 0 0 0 1 0 1 0 -0.334425 0 1.090151 0.134604 0.645463 1.613318 0.746441 -1.176597 1 ... 0 0 0 0 0 1 0 0 0 1 2 0 2.035943 1 0.753228 2.138986 -0.623215 2.516585 0.784811 -1.176597 0 ... 1 0 0 0 0 0 0 0 0 1 3 0 0.293025 0 -0.638230 1.789720 1.681556 2.263357 1.278586 -1.176597 0 ... 1 0 0 0 0 0 0 0 0 1 4 0 -0.892158 0 1.463938 0.802798 1.333045 1.623669 -0.239791 -1.174624 1 ... 0 0 0 0 0 1 0 0 0 1 5 0 1.059909 0 -0.638230 1.457122 -0.623215 1.384764 -0.639889 -1.176597 1 ... 0 0 0 0 0 1 0 0 1 0 6 0 -0.194991 0 0.744775 1.851696 -0.083403 -0.664376 -0.639889 -1.172651 1 ... 0 0 0 0 0 1 0 0 0 1 7 1 -0.055558 0 -0.638230 -0.650345 -0.623215 -0.664376 -0.639889 -1.176597 1 ... 0 0 0 0 0 0 1 0 0 1 8 0 0.432459 0 -0.638230 1.622414 0.502273 1.277608 -0.639889 -1.170678 1 ... 0 0 0 0 0 1 0 0 0 1 9 1 -1.031592 0 -0.638230 -0.650345 -0.623215 -0.664376 -0.639889 -1.174624 0 ... 0 1 0 0 0 0 0 0 1 0 <p>10 rows \u00d7 25 columns</p>"},{"location":"1-Data/ex03_again/#exercicios-data-science","title":"Exerc\u00edcios - Data Science","text":"<ul> <li>Exerc\u00edcio 1</li> <li>Exerc\u00edcio 2 - Neural Networks </li> <li>Exerc\u00edcio 3</li> </ul>"},{"location":"2-Perceptron/perceptron/","title":"Exercise 1","text":""},{"location":"2-Perceptron/perceptron/#exercise-1","title":"Exercise 1","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nmean_0 = [0, 0]\ncov_0 = [[0.5, 0], [0, 0.5]]\n\nmean_1 = [3, 3]\ncov_1 = [[0.5, 0], [0, 0.5]]\n\nclass_0 = np.random.multivariate_normal(mean_0, cov_0, 1000)\nclass_1 = np.random.multivariate_normal(mean_1, cov_1, 1000)\n\nplt.figure(figsize=(10, 8))\nplt.scatter(class_0[:, 0], class_0[:, 1], c='blue', alpha=0.6, label='Classe 0', s=20)\nplt.scatter(class_1[:, 0], class_1[:, 1], c='red', alpha=0.6, label='Classe 1', s=20)\n\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Dados Gerados - Distribui\u00e7\u00f5es Normais Multivariadas')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.axis('equal')\nplt.show()\n\nprint(\"Estat\u00edsticas da Classe 0:\")\nprint(f\"M\u00e9dia: {np.mean(class_0, axis=0)}\")\nprint(f\"Matriz de Covari\u00e2ncia:\\n{np.cov(class_0.T)}\")\n\nprint(\"\\nEstat\u00edsticas da Classe 1:\")\nprint(f\"M\u00e9dia: {np.mean(class_1, axis=0)}\")\nprint(f\"Matriz de Covari\u00e2ncia:\\n{np.cov(class_1.T)}\")\n</code></pre> <pre><code>Estat\u00edsticas da Classe 0:\nM\u00e9dia: [-0.00518042  0.0281636 ]\nMatriz de Covari\u00e2ncia:\n[[0.4938596  0.02465815]\n [0.02465815 0.4724993 ]]\n\nEstat\u00edsticas da Classe 1:\nM\u00e9dia: [3.00209722 3.0015421 ]\nMatriz de Covari\u00e2ncia:\n[[0.52323099 0.00376084]\n [0.00376084 0.51410261]]\n</code></pre> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nX = np.vstack([class_0, class_1])\ny = np.hstack([np.zeros(1000), np.ones(1000)])\n\nindices = np.random.permutation(len(X))\nX = X[indices]\ny = y[indices]\n\nw = np.random.normal(0, 0.01, 2)  \nb = 0.0  \nlearning_rate = 0.1\naccuracy_history = []\n\nprint(f\"Pesos iniciais: {w}\")\nprint(f\"Bias inicial: {b}\")\n\nfor epoch in range(100):\n    updates = 0\n\n    for i in range(len(X)):\n\n        prediction = 1 if (np.dot(w, X[i]) + b) &gt;= 0 else 0\n\n        if prediction != y[i]:\n            error = y[i] - prediction\n            w += learning_rate * error * X[i]\n            b += learning_rate * error\n            updates += 1\n\n    predictions = []\n    for i in range(len(X)):\n        pred = 1 if (np.dot(w, X[i]) + b) &gt;= 0 else 0\n        predictions.append(pred)\n    accuracy = np.mean(np.array(predictions) == y)\n    accuracy_history.append(accuracy)\n\n    print(f\"\u00c9poca {epoch+1}: Acur\u00e1cia = {accuracy:.4f}\")\n\n    if updates == 0:\n        print(f\"Converg\u00eancia na \u00e9poca {epoch+1}\")\n        break\n\nprint(f\"\\nPesos finais: {w}\")\nprint(f\"Bias final: {b}\")\nprint(f\"Acur\u00e1cia final: {accuracy:.4f}\")\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\n\nclass_0_idx = y == 0\nclass_1_idx = y == 1\nplt.scatter(X[class_0_idx, 0], X[class_0_idx, 1], c='blue', alpha=0.6, label='Classe 0')\nplt.scatter(X[class_1_idx, 0], X[class_1_idx, 1], c='red', alpha=0.6, label='Classe 1')\n\n\nx1_range = np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 100)\nx2_boundary = -(w[0] * x1_range + b) / w[1]\nplt.plot(x1_range, x2_boundary, 'k--', label='Fronteira de Decis\u00e3o')\n\nplt.xlabel('X1')\nplt.ylabel('X2')\nplt.title('Dados e Fronteira de Decis\u00e3o')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, len(accuracy_history)+1), accuracy_history, 'b-')\nplt.xlabel('\u00c9poca')\nplt.ylabel('Acur\u00e1cia')\nplt.title('Converg\u00eancia do Treinamento')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <pre><code>Pesos iniciais: [-0.0073947   0.02026473]\nBias inicial: 0.0\n\u00c9poca 1: Acur\u00e1cia = 0.9890\n\u00c9poca 2: Acur\u00e1cia = 0.9920\n\u00c9poca 3: Acur\u00e1cia = 0.9945\n\u00c9poca 4: Acur\u00e1cia = 0.9950\n\u00c9poca 5: Acur\u00e1cia = 0.9950\n\u00c9poca 6: Acur\u00e1cia = 0.9950\n\u00c9poca 7: Acur\u00e1cia = 0.9950\n\u00c9poca 8: Acur\u00e1cia = 0.9950\n\u00c9poca 9: Acur\u00e1cia = 0.9960\n\u00c9poca 10: Acur\u00e1cia = 0.9940\n\u00c9poca 11: Acur\u00e1cia = 0.9940\n\u00c9poca 12: Acur\u00e1cia = 0.9945\n\u00c9poca 13: Acur\u00e1cia = 0.9970\n\u00c9poca 14: Acur\u00e1cia = 0.9950\n\u00c9poca 15: Acur\u00e1cia = 0.9945\n\u00c9poca 16: Acur\u00e1cia = 0.9945\n\u00c9poca 17: Acur\u00e1cia = 0.9945\n\u00c9poca 18: Acur\u00e1cia = 0.9945\n\u00c9poca 19: Acur\u00e1cia = 0.9945\n\u00c9poca 20: Acur\u00e1cia = 0.9945\n\u00c9poca 21: Acur\u00e1cia = 0.9935\n\u00c9poca 22: Acur\u00e1cia = 0.9935\n\u00c9poca 23: Acur\u00e1cia = 0.9965\n\u00c9poca 24: Acur\u00e1cia = 0.9965\n\u00c9poca 25: Acur\u00e1cia = 0.9970\n\u00c9poca 26: Acur\u00e1cia = 0.9965\n\u00c9poca 27: Acur\u00e1cia = 0.9950\n\u00c9poca 28: Acur\u00e1cia = 0.9950\n\u00c9poca 29: Acur\u00e1cia = 0.9965\n\u00c9poca 30: Acur\u00e1cia = 0.9955\n\u00c9poca 31: Acur\u00e1cia = 0.9960\n\u00c9poca 32: Acur\u00e1cia = 0.9960\n\u00c9poca 33: Acur\u00e1cia = 0.9950\n\u00c9poca 34: Acur\u00e1cia = 0.9970\n\u00c9poca 35: Acur\u00e1cia = 0.9970\n\u00c9poca 36: Acur\u00e1cia = 0.9965\n\u00c9poca 37: Acur\u00e1cia = 0.9940\n\u00c9poca 38: Acur\u00e1cia = 0.9950\n\u00c9poca 39: Acur\u00e1cia = 0.9970\n\u00c9poca 40: Acur\u00e1cia = 0.9970\n\u00c9poca 41: Acur\u00e1cia = 0.9970\n\u00c9poca 42: Acur\u00e1cia = 0.9970\n\u00c9poca 43: Acur\u00e1cia = 0.9970\n\u00c9poca 44: Acur\u00e1cia = 0.9970\n\u00c9poca 45: Acur\u00e1cia = 0.9970\n\u00c9poca 46: Acur\u00e1cia = 0.9965\n\u00c9poca 47: Acur\u00e1cia = 0.9970\n\u00c9poca 48: Acur\u00e1cia = 0.9965\n\u00c9poca 49: Acur\u00e1cia = 0.9970\n\u00c9poca 50: Acur\u00e1cia = 0.9965\n\u00c9poca 51: Acur\u00e1cia = 0.9940\n\u00c9poca 52: Acur\u00e1cia = 0.9955\n\u00c9poca 53: Acur\u00e1cia = 0.9970\n\u00c9poca 54: Acur\u00e1cia = 0.9970\n\u00c9poca 55: Acur\u00e1cia = 0.9970\n\u00c9poca 56: Acur\u00e1cia = 0.9970\n\u00c9poca 57: Acur\u00e1cia = 0.9970\n\u00c9poca 58: Acur\u00e1cia = 0.9970\n\u00c9poca 59: Acur\u00e1cia = 0.9970\n\u00c9poca 60: Acur\u00e1cia = 0.9965\n\u00c9poca 61: Acur\u00e1cia = 0.9970\n\u00c9poca 62: Acur\u00e1cia = 0.9965\n\u00c9poca 63: Acur\u00e1cia = 0.9970\n\u00c9poca 64: Acur\u00e1cia = 0.9965\n\u00c9poca 65: Acur\u00e1cia = 0.9940\n\u00c9poca 66: Acur\u00e1cia = 0.9955\n\u00c9poca 67: Acur\u00e1cia = 0.9970\n\u00c9poca 68: Acur\u00e1cia = 0.9970\n\u00c9poca 69: Acur\u00e1cia = 0.9970\n\u00c9poca 70: Acur\u00e1cia = 0.9970\n\u00c9poca 71: Acur\u00e1cia = 0.9970\n\u00c9poca 72: Acur\u00e1cia = 0.9970\n\u00c9poca 73: Acur\u00e1cia = 0.9970\n\u00c9poca 74: Acur\u00e1cia = 0.9965\n\u00c9poca 75: Acur\u00e1cia = 0.9970\n\u00c9poca 76: Acur\u00e1cia = 0.9965\n\u00c9poca 77: Acur\u00e1cia = 0.9970\n\u00c9poca 78: Acur\u00e1cia = 0.9965\n\u00c9poca 79: Acur\u00e1cia = 0.9940\n\u00c9poca 80: Acur\u00e1cia = 0.9955\n\u00c9poca 81: Acur\u00e1cia = 0.9970\n\u00c9poca 82: Acur\u00e1cia = 0.9970\n\u00c9poca 83: Acur\u00e1cia = 0.9970\n\u00c9poca 84: Acur\u00e1cia = 0.9970\n\u00c9poca 85: Acur\u00e1cia = 0.9970\n\u00c9poca 86: Acur\u00e1cia = 0.9970\n\u00c9poca 87: Acur\u00e1cia = 0.9970\n\u00c9poca 88: Acur\u00e1cia = 0.9965\n\u00c9poca 89: Acur\u00e1cia = 0.9970\n\u00c9poca 90: Acur\u00e1cia = 0.9965\n\u00c9poca 91: Acur\u00e1cia = 0.9970\n\u00c9poca 92: Acur\u00e1cia = 0.9965\n\u00c9poca 93: Acur\u00e1cia = 0.9940\n\u00c9poca 94: Acur\u00e1cia = 0.9960\n\u00c9poca 95: Acur\u00e1cia = 0.9970\n\u00c9poca 96: Acur\u00e1cia = 0.9970\n\u00c9poca 97: Acur\u00e1cia = 0.9970\n\u00c9poca 98: Acur\u00e1cia = 0.9970\n\u00c9poca 99: Acur\u00e1cia = 0.9970\n\u00c9poca 100: Acur\u00e1cia = 0.9970\n\nPesos finais: [0.4564933  0.85797627]\nBias final: -1.8000000000000005\nAcur\u00e1cia final: 0.9970\n</code></pre>"},{"location":"2-Perceptron/perceptron/#exercise-2","title":"Exercise 2","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nmean_0 = [1, 1]\ncov_0 = [[1.5, 0], [0, 1.5]] \n\nmean_1 = [2, 2] \ncov_1 = [[1.5, 0], [0, 1.5]] \n\nclass_0 = np.random.multivariate_normal(mean_0, cov_0, 1000)\nclass_1 = np.random.multivariate_normal(mean_1, cov_1, 1000)\n\nX = np.vstack([class_0, class_1])\ny = np.hstack([np.zeros(1000), np.ones(1000)])\n\nindices = np.random.permutation(len(X))\nX = X[indices]\ny = y[indices]\n\nprint(\"EXERC\u00cdCIO 2 - Dados com Sobreposi\u00e7\u00e3o\")\nprint(\"=\" * 50)\nprint(f\"Classe 0 - M\u00e9dia: {mean_0}, Covari\u00e2ncia diagonal: {cov_0[0][0]}\")\nprint(f\"Classe 1 - M\u00e9dia: {mean_1}, Covari\u00e2ncia diagonal: {cov_1[0][0]}\")\nprint(f\"Dist\u00e2ncia entre m\u00e9dias: {np.linalg.norm(np.array(mean_1) - np.array(mean_0)):.2f}\")\nprint()\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nclass_0_idx = y == 0\nclass_1_idx = y == 1\nplt.scatter(class_0[:, 0], class_0[:, 1], c='blue', alpha=0.6, label='Classe 0', s=20)\nplt.scatter(class_1[:, 0], class_1[:, 1], c='red', alpha=0.6, label='Classe 1', s=20)\nplt.xlabel('X1')\nplt.ylabel('X2')\nplt.title('Dados com Sobreposi\u00e7\u00e3o')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\ndef train_perceptron(X, y, learning_rate=0.1, max_epochs=100, verbose=True):\n    w = np.random.normal(0, 0.01, 2)\n    b = 0.0\n    accuracy_history = []\n\n    if verbose:\n        print(f\"Pesos iniciais: {w}\")\n        print(f\"Bias inicial: {b}\")\n        print(\"-\" * 30)\n\n    for epoch in range(max_epochs):\n        updates = 0\n\n\n        for i in range(len(X)):\n\n            prediction = 1 if (np.dot(w, X[i]) + b) &gt;= 0 else 0\n\n\n            if prediction != y[i]:\n                error = y[i] - prediction\n                w += learning_rate * error * X[i]\n                b += learning_rate * error\n                updates += 1\n\n\n        predictions = []\n        for i in range(len(X)):\n            pred = 1 if (np.dot(w, X[i]) + b) &gt;= 0 else 0\n            predictions.append(pred)\n        accuracy = np.mean(np.array(predictions) == y)\n        accuracy_history.append(accuracy)\n\n        if verbose and (epoch + 1) % 10 == 0:\n            print(f\"\u00c9poca {epoch+1}: Acur\u00e1cia = {accuracy:.4f}, Updates = {updates}\")\n\n\n        if updates == 0:\n            if verbose:\n                print(f\"Converg\u00eancia na \u00e9poca {epoch+1}\")\n            break\n\n    if verbose:\n        print(f\"\\nPesos finais: {w}\")\n        print(f\"Bias final: {b}\")\n        print(f\"Acur\u00e1cia final: {accuracy:.4f}\")\n\n    return w, b, accuracy_history, accuracy\n\nprint(\"TREINAMENTO \u00daNICO:\")\nw, b, accuracy_history, final_accuracy = train_perceptron(X, y)\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"M\u00daLTIPLAS EXECU\u00c7\u00d5ES (5 runs):\")\naccuracies = []\nall_histories = []\n\nfor run in range(5):\n    np.random.seed(run) \n    w_run, b_run, hist_run, acc_run = train_perceptron(X, y, verbose=False)\n    accuracies.append(acc_run)\n    all_histories.append(hist_run)\n    print(f\"Run {run+1}: Acur\u00e1cia = {acc_run:.4f}\")\n\nprint(f\"\\nAcur\u00e1cia m\u00e9dia: {np.mean(accuracies):.4f} \u00b1 {np.std(accuracies):.4f}\")\nprint(f\"Melhor acur\u00e1cia: {np.max(accuracies):.4f}\")\n\nplt.subplot(1, 2, 2)\n\nplt.scatter(X[class_0_idx, 0], X[class_0_idx, 1], c='blue', alpha=0.6, label='Classe 0', s=20)\nplt.scatter(X[class_1_idx, 0], X[class_1_idx, 1], c='red', alpha=0.6, label='Classe 1', s=20)\n\npredictions = []\nfor i in range(len(X)):\n    pred = 1 if (np.dot(w, X[i]) + b) &gt;= 0 else 0\n    predictions.append(pred)\npredictions = np.array(predictions)\nmisclassified = predictions != y\n\nplt.scatter(X[misclassified, 0], X[misclassified, 1], \n           c='black', marker='x', s=80, label='Erro de Classifica\u00e7\u00e3o')\n\nx1_range = np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 100)\nif w[1] != 0:\n    x2_boundary = -(w[0] * x1_range + b) / w[1]\n    plt.plot(x1_range, x2_boundary, 'k--', linewidth=2, label='Fronteira de Decis\u00e3o')\n\nplt.xlabel('X1')\nplt.ylabel('X2')\nplt.title('Resultado do Perceptron')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(range(1, len(accuracy_history)+1), accuracy_history, 'b-', linewidth=2)\nplt.xlabel('\u00c9poca')\nplt.ylabel('Acur\u00e1cia')\nplt.title('Converg\u00eancia - Execu\u00e7\u00e3o \u00danica')\nplt.grid(True, alpha=0.3)\nplt.ylim(0.5, 1.0)\n\nplt.subplot(1, 2, 2)\nfor i, hist in enumerate(all_histories):\n    plt.plot(range(1, len(hist)+1), hist, alpha=0.7, label=f'Run {i+1}')\nplt.xlabel('\u00c9poca')\nplt.ylabel('Acur\u00e1cia')\nplt.title('Converg\u00eancia - 5 Execu\u00e7\u00f5es')\nplt.grid(True, alpha=0.3)\nplt.ylim(0.5, 1.0)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"AN\u00c1LISE COMPARATIVA COM EXERC\u00cdCIO 1:\")\nprint(\"=\" * 60)\nprint(\"Exerc\u00edcio 1 (dados separ\u00e1veis):\")\nprint(\"- Dist\u00e2ncia entre m\u00e9dias: 4.24\")\nprint(\"- Vari\u00e2ncia: 0.5\")\nprint(\"- Esperado: Converg\u00eancia r\u00e1pida, acur\u00e1cia ~100%\")\nprint()\nprint(\"Exerc\u00edcio 2 (dados com sobreposi\u00e7\u00e3o):\")\nprint(f\"- Dist\u00e2ncia entre m\u00e9dias: {np.linalg.norm(np.array(mean_1) - np.array(mean_0)):.2f}\")\nprint(\"- Vari\u00e2ncia: 1.5\")\nprint(f\"- Resultado: Acur\u00e1cia m\u00e9dia {np.mean(accuracies):.1%}\")\nprint()\nprint(\"DIFEREN\u00c7AS OBSERVADAS:\")\nprint(\"1. Menor separabilidade leva a menor acur\u00e1cia\")\nprint(\"2. Maior variabilidade entre execu\u00e7\u00f5es\")\nprint(\"3. Poss\u00edvel n\u00e3o converg\u00eancia em 100 \u00e9pocas\")\nprint(\"4. Pontos na regi\u00e3o de sobreposi\u00e7\u00e3o s\u00e3o consistentemente mal classificados\")\n</code></pre> <pre><code>EXERC\u00cdCIO 2 - Dados com Sobreposi\u00e7\u00e3o\n==================================================\nClasse 0 - M\u00e9dia: [1, 1], Covari\u00e2ncia diagonal: 1.5\nClasse 1 - M\u00e9dia: [2, 2], Covari\u00e2ncia diagonal: 1.5\nDist\u00e2ncia entre m\u00e9dias: 1.41\n\nTREINAMENTO \u00daNICO:\nPesos iniciais: [ 0.00645038 -0.00012413]\nBias inicial: 0.0\n------------------------------\n\u00c9poca 10: Acur\u00e1cia = 0.6960, Updates = 774\n\u00c9poca 20: Acur\u00e1cia = 0.7035, Updates = 795\n\u00c9poca 30: Acur\u00e1cia = 0.6315, Updates = 767\n\u00c9poca 40: Acur\u00e1cia = 0.6565, Updates = 764\n\u00c9poca 50: Acur\u00e1cia = 0.7025, Updates = 775\n\u00c9poca 60: Acur\u00e1cia = 0.6615, Updates = 787\n\u00c9poca 70: Acur\u00e1cia = 0.6145, Updates = 791\n\u00c9poca 80: Acur\u00e1cia = 0.6965, Updates = 794\n\u00c9poca 90: Acur\u00e1cia = 0.6830, Updates = 796\n\u00c9poca 100: Acur\u00e1cia = 0.6695, Updates = 784\n\nPesos finais: [0.24102045 0.20085217]\nBias final: -0.4\nAcur\u00e1cia final: 0.6695\n\n==================================================\nM\u00daLTIPLAS EXECU\u00c7\u00d5ES (5 runs):\nRun 1: Acur\u00e1cia = 0.6880\nRun 2: Acur\u00e1cia = 0.6675\nRun 3: Acur\u00e1cia = 0.6335\nRun 4: Acur\u00e1cia = 0.7040\nRun 5: Acur\u00e1cia = 0.6855\n\nAcur\u00e1cia m\u00e9dia: 0.6757 \u00b1 0.0241\nMelhor acur\u00e1cia: 0.7040\n</code></pre> <pre><code>============================================================\nAN\u00c1LISE COMPARATIVA COM EXERC\u00cdCIO 1:\n============================================================\nExerc\u00edcio 1 (dados separ\u00e1veis):\n- Dist\u00e2ncia entre m\u00e9dias: 4.24\n- Vari\u00e2ncia: 0.5\n- Esperado: Converg\u00eancia r\u00e1pida, acur\u00e1cia ~100%\n\nExerc\u00edcio 2 (dados com sobreposi\u00e7\u00e3o):\n- Dist\u00e2ncia entre m\u00e9dias: 1.41\n- Vari\u00e2ncia: 1.5\n- Resultado: Acur\u00e1cia m\u00e9dia 67.6%\n\nDIFEREN\u00c7AS OBSERVADAS:\n1. Menor separabilidade leva a menor acur\u00e1cia\n2. Maior variabilidade entre execu\u00e7\u00f5es\n3. Poss\u00edvel n\u00e3o converg\u00eancia em 100 \u00e9pocas\n4. Pontos na regi\u00e3o de sobreposi\u00e7\u00e3o s\u00e3o consistentemente mal classificados\n</code></pre>"},{"location":"2-Perceptron/perceptron/#conclusao","title":"Conclus\u00e3o","text":""},{"location":"2-Perceptron/perceptron/#analise-dos-resultados","title":"An\u00e1lise dos Resultados","text":"<p>Esta atividade demonstrou experimentalmente as limita\u00e7\u00f5es fundamentais do perceptron em dados n\u00e3o-linearmente separ\u00e1veis, confirmando as bases te\u00f3ricas do algoritmo.</p>"},{"location":"2-Perceptron/perceptron/#exercicio-1-dados-linearmente-separaveis","title":"Exerc\u00edcio 1 - Dados Linearmente Separ\u00e1veis","text":"<p>O perceptron atingiu 99,7% de acur\u00e1cia com converg\u00eancia r\u00e1pida, demonstrando sua efic\u00e1cia em datasets linearmente separ\u00e1veis. A dist\u00e2ncia significativa entre as m\u00e9dias das classes (4,24 unidades) e baixa vari\u00e2ncia (0,5) garantiram separa\u00e7\u00e3o clara, permitindo que o algoritmo encontrasse uma fronteira de decis\u00e3o \u00f3tima.</p>"},{"location":"2-Perceptron/perceptron/#exercicio-2-dados-com-sobreposicao","title":"Exerc\u00edcio 2 - Dados com Sobreposi\u00e7\u00e3o","text":"<p>Com dados parcialmente sobrepostos, o perceptron mostrou suas limita\u00e7\u00f5es inerentes, atingindo apenas 67,6% \u00b1 2,4% de acur\u00e1cia m\u00e9dia. A proximidade das m\u00e9dias (1,41 unidades) e maior vari\u00e2ncia (1,5) criaram uma regi\u00e3o de sobreposi\u00e7\u00e3o onde nenhuma linha pode separar perfeitamente as classes.</p>"},{"location":"2-Perceptron/perceptron/#principais-observacoes","title":"Principais Observa\u00e7\u00f5es","text":""},{"location":"2-Perceptron/perceptron/#convergencia","title":"Converg\u00eancia:","text":"<p>Dados separ\u00e1veis levam \u00e0 converg\u00eancia garantida; dados sobrepostos podem n\u00e3o convergir ou oscilar indefinidamente.</p>"},{"location":"2-Perceptron/perceptron/#variabilidade","title":"Variabilidade:","text":"<p>A performance em dados n\u00e3o-separ\u00e1veis varia significativamente entre execu\u00e7\u00f5es, requerendo m\u00faltiplas inicializa\u00e7\u00f5es para an\u00e1lise robusta.</p>"},{"location":"2-Perceptron/perceptron/#regiao-de-erro","title":"Regi\u00e3o de Erro:","text":"<p>Pontos na zona de sobreposi\u00e7\u00e3o s\u00e3o consistentemente mal classificados, independente da inicializa\u00e7\u00e3o dos pesos.</p>"},{"location":"2-Perceptron/perceptron/#limitacoes-do-perceptron","title":"Limita\u00e7\u00f5es do Perceptron","text":"<p>Este experimento ilustra por que o perceptron, apesar de sua simplicidade e eleg\u00e2ncia matem\u00e1tica, foi historicamente limitado a problemas linearmente separ\u00e1veis. Estas limita\u00e7\u00f5es motivaram o desenvolvimento de redes neurais multicamadas e algoritmos mais sofisticados capazes de resolver problemas n\u00e3o-lineares. A implementa\u00e7\u00e3o from-scratch refor\u00e7ou o entendimento dos fundamentos algor\u00edtmicos, demonstrando que mesmo algoritmos simples podem ser poderosos quando aplicados aos problemas corretos.</p>"},{"location":"3-MLP/mlp/","title":"Exercicio 3 - MLP","text":""},{"location":"3-MLP/mlp/#exercicio-3-mlp","title":"Exercicio 3 - MLP","text":""},{"location":"3-MLP/mlp/#exercicio-1","title":"Exercicio 1","text":"<pre><code>import numpy as np\n\n# Given values\nx = np.array([0.5, -0.2])\ny = 1.0\nW1 = np.array([[0.3, -0.1], [0.2, 0.4]])  # 2x2 matrix\nb1 = np.array([0.1, -0.2])\nW2 = np.array([0.5, -0.3])  # 1x2 matrix (row vector)\nb2 = 0.2\neta = 0.3\n\nprint(\"=== MLP MANUAL CALCULATION ===\")\nprint(f\"Input x: {x}\")\nprint(f\"Target y: {y}\")\nprint(f\"W1 (hidden layer weights):\\n{W1}\")\nprint(f\"b1 (hidden layer biases): {b1}\")\nprint(f\"W2 (output layer weights): {W2}\")\nprint(f\"b2 (output layer bias): {b2}\")\nprint(f\"Learning rate \u03b7: {eta}\")\n</code></pre> <pre><code>=== MLP MANUAL CALCULATION ===\nInput x: [ 0.5 -0.2]\nTarget y: 1.0\nW1 (hidden layer weights):\n[[ 0.3 -0.1]\n [ 0.2  0.4]]\nb1 (hidden layer biases): [ 0.1 -0.2]\nW2 (output layer weights): [ 0.5 -0.3]\nb2 (output layer bias): 0.2\nLearning rate \u03b7: 0.3\n</code></pre> <pre><code>print(\"\\n=== 1. FORWARD PASS ===\")\n\n# Step 1: Compute hidden layer pre-activations z(1) = W(1) @ x + b(1)\nprint(\"\\n1.1 Hidden layer pre-activations:\")\nprint(f\"z(1) = W(1) @ x + b(1)\")\nprint(f\"z(1) = {W1} @ {x} + {b1}\")\n\n# Matrix multiplication W(1) @ x\nWx = W1 @ x\nprint(f\"W(1) @ x = {Wx}\")\n\nz1 = Wx + b1\nprint(f\"z(1) = {Wx} + {b1} = {z1}\")\n\n# Step 2: Apply tanh to get hidden activations h(1) = tanh(z(1))\nprint(\"\\n1.2 Hidden layer activations:\")\nh1 = np.tanh(z1)\nprint(f\"h(1) = tanh(z(1)) = tanh({z1}) = {h1}\")\n\n# Step 3: Compute output pre-activation u(2) = W(2) @ h(1) + b(2)\nprint(\"\\n1.3 Output layer pre-activation:\")\nprint(f\"u(2) = W(2) @ h(1) + b(2)\")\nprint(f\"u(2) = {W2} @ {h1} + {b2}\")\n\nWh = W2 @ h1\nprint(f\"W(2) @ h(1) = {Wh}\")\n\nu2 = Wh + b2\nprint(f\"u(2) = {Wh} + {b2} = {u2}\")\n\n# Step 4: Compute final output \u0177 = tanh(u(2))\nprint(\"\\n1.4 Final output:\")\ny_hat = np.tanh(u2)\nprint(f\"\u0177 = tanh(u(2)) = tanh({u2}) = {y_hat}\")\n\nprint(f\"\\nForward pass complete: \u0177 = {y_hat:.4f}\")\n</code></pre> <pre><code>=== 1. FORWARD PASS ===\n\n1.1 Hidden layer pre-activations:\nz(1) = W(1) @ x + b(1)\nz(1) = [[ 0.3 -0.1]\n [ 0.2  0.4]] @ [ 0.5 -0.2] + [ 0.1 -0.2]\nW(1) @ x = [0.17 0.02]\nz(1) = [0.17 0.02] + [ 0.1 -0.2] = [ 0.27 -0.18]\n\n1.2 Hidden layer activations:\nh(1) = tanh(z(1)) = tanh([ 0.27 -0.18]) = [ 0.26362484 -0.17808087]\n\n1.3 Output layer pre-activation:\nu(2) = W(2) @ h(1) + b(2)\nu(2) = [ 0.5 -0.3] @ [ 0.26362484 -0.17808087] + 0.2\nW(2) @ h(1) = 0.18523667817130074\nu(2) = 0.18523667817130074 + 0.2 = 0.38523667817130075\n\n1.4 Final output:\n\u0177 = tanh(u(2)) = tanh(0.38523667817130075) = 0.36724656264510797\n\nForward pass complete: \u0177 = 0.3672\n</code></pre> <pre><code>print(\"\\n=== 2. LOSS CALCULATION ===\")\n\n# Compute MSE loss: L = 1/N * (y - \u0177)\u00b2\n# Since N = 1 (single sample), L = (y - \u0177)\u00b2\nerror = y - y_hat\nloss = error**2  # Standard MSE\nprint(f\"Error = y - \u0177 = {y} - {y_hat:.4f} = {error:.4f}\")\nprint(f\"MSE Loss = (y - \u0177)\u00b2 = ({error:.4f})\u00b2 = {loss:.4f}\")\n</code></pre> <pre><code>=== 2. LOSS CALCULATION ===\nError = y - \u0177 = 1.0 - 0.3672 = 0.6328\nMSE Loss = (y - \u0177)\u00b2 = (0.6328)\u00b2 = 0.4004\n</code></pre> <pre><code>print(\"\\n=== 3. BACKWARD PASS (BACKPROPAGATION) ===\")\n\n# Start with \u2202L/\u2202\u0177\nprint(\"\\n3.1 Gradient with respect to output:\")\ndL_dy_hat = -2 * (y - y_hat)  # For L = (y - \u0177)\u00b2, \u2202L/\u2202\u0177 = -2(y - \u0177)\nprint(f\"\u2202L/\u2202\u0177 = -2(y - \u0177) = -2({y} - {y_hat:.4f}) = {dL_dy_hat:.4f}\")\n\n# \u2202L/\u2202u(2) using tanh derivative: d/du tanh(u) = 1 - tanh\u00b2(u)\nprint(\"\\n3.2 Gradient with respect to output pre-activation:\")\ndtanh_du2 = 1 - np.tanh(u2)**2\ndL_du2 = dL_dy_hat * dtanh_du2\nprint(f\"\u2202\u0177/\u2202u(2) = 1 - tanh\u00b2(u(2)) = 1 - tanh\u00b2({u2:.4f}) = 1 - ({np.tanh(u2):.4f})\u00b2 = {dtanh_du2:.4f}\")\nprint(f\"\u2202L/\u2202u(2) = \u2202L/\u2202\u0177 * \u2202\u0177/\u2202u(2) = {dL_dy_hat:.4f} * {dtanh_du2:.4f} = {dL_du2:.4f}\")\n\n# Gradients for output layer: \u2202L/\u2202W(2), \u2202L/\u2202b(2)\nprint(\"\\n3.3 Gradients for output layer:\")\ndL_dW2 = dL_du2 * h1\ndL_db2 = dL_du2\nprint(f\"\u2202L/\u2202W(2) = \u2202L/\u2202u(2) * h(1) = {dL_du2:.4f} * {h1} = {dL_dW2}\")\nprint(f\"\u2202L/\u2202b(2) = \u2202L/\u2202u(2) = {dL_db2:.4f}\")\n\n# Propagate to hidden layer: \u2202L/\u2202h(1), \u2202L/\u2202z(1)\nprint(\"\\n3.4 Propagate to hidden layer:\")\ndL_dh1 = dL_du2 * W2\nprint(f\"\u2202L/\u2202h(1) = \u2202L/\u2202u(2) * W(2) = {dL_du2:.4f} * {W2} = {dL_dh1}\")\n\ndtanh_dz1 = 1 - np.tanh(z1)**2\ndL_dz1 = dL_dh1 * dtanh_dz1\nprint(f\"\u2202h(1)/\u2202z(1) = 1 - tanh\u00b2(z(1)) = 1 - tanh\u00b2({z1}) = 1 - {np.tanh(z1)**2} = {dtanh_dz1}\")\nprint(f\"\u2202L/\u2202z(1) = \u2202L/\u2202h(1) * \u2202h(1)/\u2202z(1) = {dL_dh1} * {dtanh_dz1} = {dL_dz1}\")\n\n# Gradients for hidden layer: \u2202L/\u2202W(1), \u2202L/\u2202b(1)\nprint(\"\\n3.5 Gradients for hidden layer:\")\ndL_dW1 = np.outer(dL_dz1, x)\ndL_db1 = dL_dz1\nprint(f\"\u2202L/\u2202W(1) = \u2202L/\u2202z(1) \u2297 x = {dL_dz1} \u2297 {x}\")\nprint(f\"\u2202L/\u2202W(1) =\")\nprint(dL_dW1)\nprint(f\"\u2202L/\u2202b(1) = \u2202L/\u2202z(1) = {dL_db1}\")\n</code></pre> <pre><code>=== 3. BACKWARD PASS (BACKPROPAGATION) ===\n\n3.1 Gradient with respect to output:\n\u2202L/\u2202\u0177 = -2(y - \u0177) = -2(1.0 - 0.3672) = -1.2655\n\n3.2 Gradient with respect to output pre-activation:\n\u2202\u0177/\u2202u(2) = 1 - tanh\u00b2(u(2)) = 1 - tanh\u00b2(0.3852) = 1 - (0.3672)\u00b2 = 0.8651\n\u2202L/\u2202u(2) = \u2202L/\u2202\u0177 * \u2202\u0177/\u2202u(2) = -1.2655 * 0.8651 = -1.0948\n\n3.3 Gradients for output layer:\n\u2202L/\u2202W(2) = \u2202L/\u2202u(2) * h(1) = -1.0948 * [ 0.26362484 -0.17808087] = [-0.28862383  0.19496791]\n\u2202L/\u2202b(2) = \u2202L/\u2202u(2) = -1.0948\n\n3.4 Propagate to hidden layer:\n\u2202L/\u2202h(1) = \u2202L/\u2202u(2) * W(2) = -1.0948 * [ 0.5 -0.3] = [-0.54741396  0.32844837]\n\u2202h(1)/\u2202z(1) = 1 - tanh\u00b2(z(1)) = 1 - tanh\u00b2([ 0.27 -0.18]) = 1 - [0.06949805 0.0317128 ] = [0.93050195 0.9682872 ]\n\u2202L/\u2202z(1) = \u2202L/\u2202h(1) * \u2202h(1)/\u2202z(1) = [-0.54741396  0.32844837] * [0.93050195 0.9682872 ] = [-0.50936975  0.31803236]\n\n3.5 Gradients for hidden layer:\n\u2202L/\u2202W(1) = \u2202L/\u2202z(1) \u2297 x = [-0.50936975  0.31803236] \u2297 [ 0.5 -0.2]\n\u2202L/\u2202W(1) =\n[[-0.25468488  0.10187395]\n [ 0.15901618 -0.06360647]]\n\u2202L/\u2202b(1) = \u2202L/\u2202z(1) = [-0.50936975  0.31803236]\n</code></pre> <pre><code>print(\"\\n=== 4. PARAMETER UPDATE ===\")\n\nprint(\"\\n4.1 Update weights and biases using gradient descent:\")\nprint(f\"Learning rate \u03b7 = {eta}\")\n\n# Store original parameters for comparison\nW1_original = W1.copy()\nb1_original = b1.copy()\nW2_original = W2.copy()\nb2_original = b2\n\n# Update parameters: param = param - \u03b7 * gradient\nW1_new = W1 - eta * dL_dW1\nb1_new = b1 - eta * dL_db1\nW2_new = W2 - eta * dL_dW2\nb2_new = b2 - eta * dL_db2\n\nprint(f\"\\n4.2 Updated parameters:\")\nprint(f\"W(1) \u2190 W(1) - \u03b7 * \u2202L/\u2202W(1)\")\nprint(f\"W(1) \u2190 {W1_original} - {eta} * {dL_dW1}\")\nprint(f\"W(1) = {W1_new}\")\n\nprint(f\"\\nb(1) \u2190 b(1) - \u03b7 * \u2202L/\u2202b(1)\")\nprint(f\"b(1) \u2190 {b1_original} - {eta} * {dL_db1}\")\nprint(f\"b(1) = {b1_new}\")\n\nprint(f\"\\nW(2) \u2190 W(2) - \u03b7 * \u2202L/\u2202W(2)\")\nprint(f\"W(2) \u2190 {W2_original} - {eta} * {dL_dW2}\")\nprint(f\"W(2) = {W2_new}\")\n\nprint(f\"\\nb(2) \u2190 b(2) - \u03b7 * \u2202L/\u2202b(2)\")\nprint(f\"b(2) \u2190 {b2_original} - {eta} * {dL_db2}\")\nprint(f\"b(2) = {b2_new}\")\n</code></pre> <pre><code>=== 4. PARAMETER UPDATE ===\n\n4.1 Update weights and biases using gradient descent:\nLearning rate \u03b7 = 0.3\n\n4.2 Updated parameters:\nW(1) \u2190 W(1) - \u03b7 * \u2202L/\u2202W(1)\nW(1) \u2190 [[ 0.3 -0.1]\n [ 0.2  0.4]] - 0.3 * [[-0.25468488  0.10187395]\n [ 0.15901618 -0.06360647]]\nW(1) = [[ 0.37640546 -0.13056219]\n [ 0.15229515  0.41908194]]\n\nb(1) \u2190 b(1) - \u03b7 * \u2202L/\u2202b(1)\nb(1) \u2190 [ 0.1 -0.2] - 0.3 * [-0.50936975  0.31803236]\nb(1) = [ 0.25281093 -0.29540971]\n\nW(2) \u2190 W(2) - \u03b7 * \u2202L/\u2202W(2)\nW(2) \u2190 [ 0.5 -0.3] - 0.3 * [-0.28862383  0.19496791]\nW(2) = [ 0.58658715 -0.35849037]\n\nb(2) \u2190 b(2) - \u03b7 * \u2202L/\u2202b(2)\nb(2) \u2190 0.2 - 0.3 * -1.0948279147135995\nb(2) = 0.5284483744140799\n</code></pre> <pre><code>print(\"\\n=== SUMMARY OF RESULTS ===\")\nprint(f\"\\nOriginal Loss: {loss:.4f}\")\nprint(f\"Forward pass output: \u0177 = {y_hat:.4f}\")\nprint(f\"Target output: y = {y}\")\nprint(f\"Error: {error:.4f}\")\n\nprint(f\"\\nUpdated Parameters:\")\nprint(f\"W(1) = \\n{W1_new}\")\nprint(f\"b(1) = {b1_new}\")\nprint(f\"W(2) = {W2_new}\")\nprint(f\"b(2) = {b2_new:.4f}\")\n</code></pre> <pre><code>=== SUMMARY OF RESULTS ===\n\nOriginal Loss: 0.4004\nForward pass output: \u0177 = 0.3672\nTarget output: y = 1.0\nError: 0.6328\n\nUpdated Parameters:\nW(1) = \n[[ 0.37640546 -0.13056219]\n [ 0.15229515  0.41908194]]\nb(1) = [ 0.25281093 -0.29540971]\nW(2) = [ 0.58658715 -0.35849037]\nb(2) = 0.5284\n</code></pre>"},{"location":"3-MLP/mlp/#exercicio-2","title":"Exercicio 2","text":"<pre><code>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=== EXERCISE 2: BINARY CLASSIFICATION WITH SYNTHETIC DATA ===\")\n\n# Generate synthetic dataset with different clusters per class\nnp.random.seed(42)\n\n# Generate class 0 with 1 cluster (500 samples)\nX_class0, _ = make_classification(n_samples=500, n_features=2, n_informative=2, \n                                  n_redundant=0, n_clusters_per_class=1, \n                                  random_state=42, class_sep=1.2)\ny_class0 = np.zeros(500, dtype=int)\n\n# Generate class 1 with 2 clusters (500 samples) \nX_class1, _ = make_classification(n_samples=500, n_features=2, n_informative=2,\n                                  n_redundant=0, n_clusters_per_class=2,\n                                  random_state=123, class_sep=1.0)\ny_class1 = np.ones(500, dtype=int)\n\n# Combine the datasets\nX = np.vstack([X_class0, X_class1])\ny = np.hstack([y_class0, y_class1])\n\n# Shuffle the combined dataset\nindices = np.random.permutation(len(X))\nX = X[indices]\ny = y[indices]\n\n# Split into training and testing sets (80%-20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n                                                    random_state=42, stratify=y)\n\nprint(f\"Dataset created:\")\nprint(f\"Total samples: {len(X)}\")\nprint(f\"Features: {X.shape[1]}\")\nprint(f\"Class distribution: {np.bincount(y)}\")\nprint(f\"\\nData split:\")\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Testing samples: {len(X_test)}\")\n</code></pre> <pre><code>=== EXERCISE 2: BINARY CLASSIFICATION WITH SYNTHETIC DATA ===\nDataset created:\nTotal samples: 1000\nFeatures: 2\nClass distribution: [500 500]\n\nData split:\nTraining samples: 800\nTesting samples: 200\n</code></pre> <pre><code>class MLP:\n    def __init__(self, input_size=2, hidden_sizes=[10, 8], output_size=1, learning_rate=0.01):\n        \"\"\"Initialize MLP with specified architecture\"\"\"\n        self.learning_rate = learning_rate\n\n        # Create layer sizes list\n        layer_sizes = [input_size] + hidden_sizes + [output_size]\n\n        # Initialize weights and biases\n        self.weights = []\n        self.biases = []\n\n        for i in range(len(layer_sizes) - 1):\n            # Xavier initialization\n            weight = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])\n            bias = np.zeros((1, layer_sizes[i+1]))\n\n            self.weights.append(weight)\n            self.biases.append(bias)\n\n        self.num_layers = len(self.weights)\n        print(f\"MLP Architecture: {layer_sizes}\")\n\n    def sigmoid(self, z):\n        \"\"\"Sigmoid activation function\"\"\"\n        z = np.clip(z, -250, 250)\n        return 1 / (1 + np.exp(-z))\n\n    def relu(self, z):\n        \"\"\"ReLU activation function\"\"\"\n        return np.maximum(0, z)\n\n    def relu_derivative(self, z):\n        \"\"\"Derivative of ReLU function\"\"\"\n        return (z &gt; 0).astype(float)\n\n    def forward_pass(self, X):\n        \"\"\"Forward pass through the network\"\"\"\n        self.activations = [X]\n        self.z_values = []\n\n        current_input = X\n\n        # Forward through all layers\n        for i in range(self.num_layers):\n            z = np.dot(current_input, self.weights[i]) + self.biases[i]\n            self.z_values.append(z)\n\n            if i == self.num_layers - 1:\n                # Output layer with sigmoid\n                current_input = self.sigmoid(z)\n            else:\n                # Hidden layers with ReLU\n                current_input = self.relu(z)\n\n            self.activations.append(current_input)\n\n        return current_input\n\n    def compute_loss(self, y_true, y_pred):\n        \"\"\"Compute binary cross-entropy loss\"\"\"\n        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n        return loss\n\n    def backward_pass(self, X, y_true, y_pred):\n        \"\"\"Backward pass (backpropagation) - CORRECTED VERSION\"\"\"\n        m = X.shape[0]\n\n        # Initialize gradients\n        dW = [np.zeros_like(w) for w in self.weights]\n        db = [np.zeros_like(b) for b in self.biases]\n\n        # Output layer gradient (binary cross-entropy + sigmoid)\n        dz = y_pred - y_true.reshape(-1, 1)\n\n        # Backpropagate through layers\n        for i in reversed(range(self.num_layers)):\n            # Compute gradients for current layer\n            dW[i] = np.dot(self.activations[i].T, dz) / m\n            db[i] = np.sum(dz, axis=0, keepdims=True) / m\n\n            # Propagate error to previous layer (if not input layer)\n            if i &gt; 0:\n                dz = np.dot(dz, self.weights[i].T) * self.relu_derivative(self.z_values[i-1])\n\n        return dW, db\n\n    def update_parameters(self, dW, db):\n        \"\"\"Update weights and biases\"\"\"\n        for i in range(self.num_layers):\n            self.weights[i] -= self.learning_rate * dW[i]\n            self.biases[i] -= self.learning_rate * db[i]\n\n    def train(self, X, y, epochs=300, verbose=True):\n        \"\"\"Train the MLP\"\"\"\n        self.loss_history = []\n\n        for epoch in range(epochs):\n            # Forward pass\n            y_pred = self.forward_pass(X)\n\n            # Compute loss\n            loss = self.compute_loss(y, y_pred)\n            self.loss_history.append(loss)\n\n            # Backward pass\n            dW, db = self.backward_pass(X, y, y_pred)\n\n            # Update parameters\n            self.update_parameters(dW, db)\n\n            # Print progress\n            if verbose and (epoch + 1) % 50 == 0:\n                accuracy = self.evaluate(X, y)\n                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n\n        return self.loss_history\n\n    def predict(self, X):\n        \"\"\"Make predictions\"\"\"\n        y_pred = self.forward_pass(X)\n        return (y_pred &gt;= 0.5).astype(int).flatten()\n\n    def predict_proba(self, X):\n        \"\"\"Get prediction probabilities\"\"\"\n        return self.forward_pass(X).flatten()\n\n    def evaluate(self, X, y):\n        \"\"\"Evaluate model accuracy\"\"\"\n        predictions = self.predict(X)\n        accuracy = np.mean(predictions == y)\n        return accuracy\n\nprint(\"MLP class implemented successfully!\")\n</code></pre> <pre><code>MLP class implemented successfully!\n</code></pre> <pre><code># Initialize and train the MLP\nprint(\"=== TRAINING THE MLP ===\")\nmlp = MLP(input_size=2, hidden_sizes=[16, 8], output_size=1, learning_rate=0.1)\n\n# Train the model\nloss_history = mlp.train(X_train, y_train, epochs=300, verbose=True)\n\nprint(f\"\\nTraining completed!\")\nprint(f\"Final training loss: {loss_history[-1]:.4f}\")\n</code></pre> <pre><code>=== TRAINING THE MLP ===\nMLP Architecture: [2, 16, 8, 1]\nEpoch 50/300, Loss: 0.8477, Accuracy: 0.7400\nEpoch 100/300, Loss: 0.9468, Accuracy: 0.7725\nEpoch 150/300, Loss: 1.0344, Accuracy: 0.7887\nEpoch 200/300, Loss: 1.1030, Accuracy: 0.7925\nEpoch 250/300, Loss: 1.1181, Accuracy: 0.8000\nEpoch 300/300, Loss: 1.1290, Accuracy: 0.8087\n\nTraining completed!\nFinal training loss: 1.1290\n</code></pre> <pre><code># Evaluate on training and test sets\nprint(\"=== MODEL EVALUATION ===\")\n\n# Training accuracy\ntrain_accuracy = mlp.evaluate(X_train, y_train)\nprint(f\"Training Accuracy: {train_accuracy:.4f}\")\n\n# Test accuracy\ntest_accuracy = mlp.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n\n# Predictions on test set - CORRE\u00c7\u00c3O AQUI: usar X_test, n\u00e3o y_test\ny_pred_test = mlp.predict(X_test)  # CORRIGIDO: era mlp.predict(y_test)\ny_proba_test = mlp.predict_proba(X_test)\n\n# Confusion Matrix (manual implementation)\nTP = np.sum((y_test == 1) &amp; (y_pred_test == 1))\nTN = np.sum((y_test == 0) &amp; (y_pred_test == 0))\nFP = np.sum((y_test == 0) &amp; (y_pred_test == 1))\nFN = np.sum((y_test == 1) &amp; (y_pred_test == 0))\n\nprint(f\"\\nConfusion Matrix:\")\nprint(f\"True Negatives: {TN}, False Positives: {FP}\")\nprint(f\"False Negatives: {FN}, True Positives: {TP}\")\n\n# Additional metrics\nprecision = TP / (TP + FP) if (TP + FP) &gt; 0 else 0\nrecall = TP / (TP + FN) if (TP + FN) &gt; 0 else 0\nf1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) &gt; 0 else 0\n\nprint(f\"\\nAdditional Metrics:\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-Score: {f1_score:.4f}\")\n\n# Show some sample predictions\nprint(f\"\\nSample predictions vs actual:\")\nfor i in range(10):\n    print(f\"Sample {i+1}: Predicted={y_pred_test[i]}, Actual={y_test[i]}, Probability={y_proba_test[i]:.4f}\")\n</code></pre> <pre><code>=== MODEL EVALUATION ===\nTraining Accuracy: 0.8087\nTest Accuracy: 0.7650\n\nConfusion Matrix:\nTrue Negatives: 92, False Positives: 8\nFalse Negatives: 39, True Positives: 61\n\nAdditional Metrics:\nPrecision: 0.8841\nRecall: 0.6100\nF1-Score: 0.7219\n\nSample predictions vs actual:\nSample 1: Predicted=1, Actual=1, Probability=0.9911\nSample 2: Predicted=0, Actual=0, Probability=0.2895\nSample 3: Predicted=0, Actual=0, Probability=0.2663\nSample 4: Predicted=0, Actual=0, Probability=0.1172\nSample 5: Predicted=0, Actual=1, Probability=0.3086\nSample 6: Predicted=1, Actual=1, Probability=0.9742\nSample 7: Predicted=1, Actual=1, Probability=0.9299\nSample 8: Predicted=1, Actual=1, Probability=0.9705\nSample 9: Predicted=0, Actual=0, Probability=0.2987\nSample 10: Predicted=0, Actual=0, Probability=0.3852\n</code></pre> <pre><code># Plot training loss\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 4))\n\n# Plot 1: Training Loss\nplt.subplot(1, 2, 1)\nplt.plot(loss_history)\nplt.title('Training Loss Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Binary Cross-Entropy Loss')\nplt.grid(True)\n\n# Plot 2: Data Visualization with Predictions\nplt.subplot(1, 2, 2)\n# Plot training data\ncolors = ['red' if label == 0 else 'blue' for label in y_test]\nplt.scatter(X_test[:, 0], X_test[:, 1], c=colors, alpha=0.7, s=50)\n\n# Add predictions as edge colors\npred_colors = ['red' if pred == 0 else 'blue' for pred in y_pred_test]\nplt.scatter(X_test[:, 0], X_test[:, 1], c='none', edgecolors=pred_colors, \n           linewidth=2, s=50, alpha=0.8)\n\nplt.title('Test Data: Actual (fill) vs Predicted (edge)')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend(['Class 0', 'Class 1'])\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Visualization complete!\")\nprint(\"Red = Class 0, Blue = Class 1\")\nprint(\"Fill color = actual label, Edge color = predicted label\")\n</code></pre> <pre><code>Visualization complete!\nRed = Class 0, Blue = Class 1\nFill color = actual label, Edge color = predicted label\n</code></pre>"},{"location":"3-MLP/mlp/#exercicio-3","title":"Exercicio 3","text":"<pre><code>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=== EXERCISE 3: MULTI-CLASS CLASSIFICATION ===\")\n\n# Generate synthetic dataset with different clusters per class\nnp.random.seed(42)\n\n# Class 0 with 2 clusters (500 samples)\nX_class0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, \n                                  n_redundant=0, n_clusters_per_class=2, \n                                  random_state=42, class_sep=1.5)\ny_class0 = np.zeros(500, dtype=int)\n\n# Class 1 with 3 clusters (500 samples)\nX_class1, _ = make_classification(n_samples=500, n_features=4, n_informative=4,\n                                  n_redundant=0, n_clusters_per_class=3,\n                                  random_state=123, class_sep=1.2)\ny_class1 = np.ones(500, dtype=int)\n\n# Class 2 with 4 clusters (500 samples)\nX_class2, _ = make_classification(n_samples=500, n_features=4, n_informative=4,\n                                  n_redundant=0, n_clusters_per_class=4,\n                                  random_state=456, class_sep=1.0)\ny_class2 = np.full(500, 2, dtype=int)\n\n# Combine the datasets\nX = np.vstack([X_class0, X_class1, X_class2])\ny = np.hstack([y_class0, y_class1, y_class2])\n\n# Shuffle the combined dataset\nindices = np.random.permutation(len(X))\nX = X[indices]\ny = y[indices]\n\n# Split into training and testing sets (80%-20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n                                                    random_state=42, stratify=y)\n\nprint(f\"Dataset created:\")\nprint(f\"Total samples: {len(X)}\")\nprint(f\"Features: {X.shape[1]}\")\nprint(f\"Classes: {len(np.unique(y))}\")\nprint(f\"Class distribution: {np.bincount(y)}\")\nprint(f\"\\nData split:\")\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Testing samples: {len(X_test)}\")\nprint(f\"Training class distribution: {np.bincount(y_train)}\")\nprint(f\"Testing class distribution: {np.bincount(y_test)}\")\n</code></pre> <pre><code>=== EXERCISE 3: MULTI-CLASS CLASSIFICATION ===\nDataset created:\nTotal samples: 1500\nFeatures: 4\nClasses: 3\nClass distribution: [500 500 500]\n\nData split:\nTraining samples: 1200\nTesting samples: 300\nTraining class distribution: [400 400 400]\nTesting class distribution: [100 100 100]\n</code></pre> <pre><code>def one_hot_encode(y, num_classes):\n    \"\"\"Convert labels to one-hot encoding\"\"\"\n    encoded = np.zeros((len(y), num_classes))\n    encoded[np.arange(len(y)), y] = 1\n    return encoded\n\ndef softmax(z):\n    \"\"\"Softmax activation function\"\"\"\n    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Numerical stability\n    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n# Convert labels to one-hot encoding\nnum_classes = 3\ny_train_onehot = one_hot_encode(y_train, num_classes)\ny_test_onehot = one_hot_encode(y_test, num_classes)\n\nprint(\"Multi-class helper functions created!\")\nprint(f\"Original y_train shape: {y_train.shape}\")\nprint(f\"One-hot y_train shape: {y_train_onehot.shape}\")\nprint(f\"Sample one-hot encoding:\")\nprint(f\"Original: {y_train[:5]}\")\nprint(f\"One-hot:\\n{y_train_onehot[:5]}\")\n</code></pre> <pre><code>Multi-class helper functions created!\nOriginal y_train shape: (1200,)\nOne-hot y_train shape: (1200, 3)\nSample one-hot encoding:\nOriginal: [0 0 0 0 2]\nOne-hot:\n[[1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 0. 1.]]\n</code></pre> <pre><code># (mesmo c\u00f3digo do ex2)\nclass MLP:\n    def __init__(self, input_size=2, hidden_sizes=[10, 8], output_size=1, learning_rate=0.01):\n        \"\"\"Initialize MLP with specified architecture\"\"\"\n        self.learning_rate = learning_rate\n\n        # Create layer sizes list\n        layer_sizes = [input_size] + hidden_sizes + [output_size]\n\n        # Initialize weights and biases\n        self.weights = []\n        self.biases = []\n\n        for i in range(len(layer_sizes) - 1):\n            # Xavier initialization\n            weight = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])\n            bias = np.zeros((1, layer_sizes[i+1]))\n\n            self.weights.append(weight)\n            self.biases.append(bias)\n\n        self.num_layers = len(self.weights)\n        print(f\"MLP Architecture: {layer_sizes}\")\n\n    def sigmoid(self, z):\n        \"\"\"Sigmoid activation function\"\"\"\n        z = np.clip(z, -250, 250)\n        return 1 / (1 + np.exp(-z))\n\n    def relu(self, z):\n        \"\"\"ReLU activation function\"\"\"\n        return np.maximum(0, z)\n\n    def relu_derivative(self, z):\n        \"\"\"Derivative of ReLU function\"\"\"\n        return (z &gt; 0).astype(float)\n\n    def forward_pass(self, X):\n        \"\"\"Forward pass through the network\"\"\"\n        self.activations = [X]\n        self.z_values = []\n\n        current_input = X\n\n        # Forward through all layers\n        for i in range(self.num_layers):\n            z = np.dot(current_input, self.weights[i]) + self.biases[i]\n            self.z_values.append(z)\n\n            if i == self.num_layers - 1:\n                # Output layer with sigmoid (will be overridden for multi-class)\n                current_input = self.sigmoid(z)\n            else:\n                # Hidden layers with ReLU\n                current_input = self.relu(z)\n\n            self.activations.append(current_input)\n\n        return current_input\n\n    def compute_loss(self, y_true, y_pred):\n        \"\"\"Compute binary cross-entropy loss\"\"\"\n        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n        return loss\n\n    def backward_pass(self, X, y_true, y_pred):\n        \"\"\"Backward pass (backpropagation) - CORRECTED VERSION\"\"\"\n        m = X.shape[0]\n\n        # Initialize gradients\n        dW = [np.zeros_like(w) for w in self.weights]\n        db = [np.zeros_like(b) for b in self.biases]\n\n        # Output layer gradient (binary cross-entropy + sigmoid)\n        dz = y_pred - y_true.reshape(-1, 1) if y_true.ndim == 1 else y_pred - y_true\n\n        # Backpropagate through layers\n        for i in reversed(range(self.num_layers)):\n            # Compute gradients for current layer\n            dW[i] = np.dot(self.activations[i].T, dz) / m\n            db[i] = np.sum(dz, axis=0, keepdims=True) / m\n\n            # Propagate error to previous layer (if not input layer)\n            if i &gt; 0:\n                dz = np.dot(dz, self.weights[i].T) * self.relu_derivative(self.z_values[i-1])\n\n        return dW, db\n\n    def update_parameters(self, dW, db):\n        \"\"\"Update weights and biases\"\"\"\n        for i in range(self.num_layers):\n            self.weights[i] -= self.learning_rate * dW[i]\n            self.biases[i] -= self.learning_rate * db[i]\n\n    def train(self, X, y, epochs=300, verbose=True):\n        \"\"\"Train the MLP\"\"\"\n        self.loss_history = []\n\n        for epoch in range(epochs):\n            # Forward pass\n            y_pred = self.forward_pass(X)\n\n            # Compute loss\n            loss = self.compute_loss(y, y_pred)\n            self.loss_history.append(loss)\n\n            # Backward pass\n            dW, db = self.backward_pass(X, y, y_pred)\n\n            # Update parameters\n            self.update_parameters(dW, db)\n\n            # Print progress\n            if verbose and (epoch + 1) % 50 == 0:\n                accuracy = self.evaluate(X, y)\n                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n\n        return self.loss_history\n\n    def predict(self, X):\n        \"\"\"Make predictions\"\"\"\n        y_pred = self.forward_pass(X)\n        if y_pred.shape[1] == 1:\n            # Binary classification\n            return (y_pred &gt;= 0.5).astype(int).flatten()\n        else:\n            # Multi-class classification\n            return np.argmax(y_pred, axis=1)\n\n    def predict_proba(self, X):\n        \"\"\"Get prediction probabilities\"\"\"\n        return self.forward_pass(X)\n\n    def evaluate(self, X, y):\n        \"\"\"Evaluate model accuracy\"\"\"\n        predictions = self.predict(X)\n        if y.ndim &gt; 1:\n            # One-hot encoded labels\n            y_labels = np.argmax(y, axis=1)\n        else:\n            y_labels = y\n        accuracy = np.mean(predictions == y_labels)\n        return accuracy\n\nprint(\"MLP class implemented successfully (same as Exercise 2)!\")\n</code></pre> <pre><code>MLP class implemented successfully (same as Exercise 2)!\n</code></pre> <pre><code>class MultiClassMLP(MLP):\n    \"\"\"Extension of MLP for multi-class classification - ONLY modifying activation and loss\"\"\"\n\n    def forward_pass(self, X):\n        \"\"\"Forward pass with softmax output for multi-class\"\"\"\n        self.activations = [X]\n        self.z_values = []\n\n        current_input = X\n\n        # Forward through all layers\n        for i in range(self.num_layers):\n            z = np.dot(current_input, self.weights[i]) + self.biases[i]\n            self.z_values.append(z)\n\n            if i == self.num_layers - 1:\n                # Output layer with softmax for multi-class\n                current_input = softmax(z)\n            else:\n                # Hidden layers with ReLU (unchanged)\n                current_input = self.relu(z)\n\n            self.activations.append(current_input)\n\n        return current_input\n\n    def compute_loss(self, y_true, y_pred):\n        \"\"\"Compute categorical cross-entropy loss\"\"\"\n        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n        loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n        return loss\n\n# Initialize multi-class MLP with ONLY hyperparameter changes\nprint(\"=== INITIALIZING MULTI-CLASS MLP ===\")\nmlp_multiclass = MultiClassMLP(\n    input_size=4,        # 4 features instead of 2\n    hidden_sizes=[20, 16], # Slightly larger for more complex problem\n    output_size=3,       # 3 classes instead of 1\n    learning_rate=0.05   # Learning rate adjustment\n)\n\nprint(\"Multi-class MLP ready!\")\n</code></pre> <pre><code>=== INITIALIZING MULTI-CLASS MLP ===\nMLP Architecture: [4, 20, 16, 3]\nMulti-class MLP ready!\n</code></pre> <pre><code># Train the multi-class MLP\nprint(\"=== TRAINING MULTI-CLASS MLP ===\")\nloss_history = mlp_multiclass.train(X_train, y_train_onehot, epochs=400, verbose=True)\n\nprint(f\"\\nTraining completed!\")\nprint(f\"Final training loss: {loss_history[-1]:.4f}\")\n</code></pre> <pre><code>=== TRAINING MULTI-CLASS MLP ===\nEpoch 50/400, Loss: 0.9226, Accuracy: 0.5667\nEpoch 100/400, Loss: 0.8604, Accuracy: 0.6125\nEpoch 150/400, Loss: 0.8264, Accuracy: 0.6325\nEpoch 200/400, Loss: 0.8026, Accuracy: 0.6350\nEpoch 250/400, Loss: 0.7833, Accuracy: 0.6450\nEpoch 300/400, Loss: 0.7662, Accuracy: 0.6542\nEpoch 350/400, Loss: 0.7519, Accuracy: 0.6575\nEpoch 400/400, Loss: 0.7391, Accuracy: 0.6633\n\nTraining completed!\nFinal training loss: 0.7391\n</code></pre> <pre><code># Evaluate multi-class model\nprint(\"=== MULTI-CLASS MODEL EVALUATION ===\")\n\n# Training accuracy\ntrain_accuracy = mlp_multiclass.evaluate(X_train, y_train_onehot)\nprint(f\"Training Accuracy: {train_accuracy:.4f}\")\n\n# Test accuracy\ntest_accuracy = mlp_multiclass.evaluate(X_test, y_test_onehot)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n\n# Predictions on test set\ny_pred_test = mlp_multiclass.predict(X_test)\ny_proba_test = mlp_multiclass.predict_proba(X_test)\n\n# Multi-class Confusion Matrix\nfrom collections import defaultdict\nconfusion_matrix = np.zeros((3, 3), dtype=int)\n\nfor true_label, pred_label in zip(y_test, y_pred_test):\n    confusion_matrix[true_label, pred_label] += 1\n\nprint(f\"\\nConfusion Matrix:\")\nprint(\"Predicted \u2192\")\nprint(\"Actual \u2193  \", end=\"\")\nprint(\"  0   1   2\")\nfor i in range(3):\n    print(f\"Class {i}:\", end=\"\")\n    for j in range(3):\n        print(f\" {confusion_matrix[i,j]:3d}\", end=\"\")\n    print()\n\n# Per-class metrics\nprint(f\"\\nPer-class metrics:\")\nfor class_idx in range(3):\n    # True positives, false positives, false negatives\n    tp = confusion_matrix[class_idx, class_idx]\n    fp = np.sum(confusion_matrix[:, class_idx]) - tp\n    fn = np.sum(confusion_matrix[class_idx, :]) - tp\n\n    precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) &gt; 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) &gt; 0 else 0\n\n    print(f\"Class {class_idx}: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}\")\n\n# Show sample predictions with probabilities\nprint(f\"\\nSample predictions:\")\nfor i in range(15):\n    pred_class = y_pred_test[i]\n    true_class = y_test[i]\n    probs = y_proba_test[i]\n    print(f\"Sample {i+1}: Predicted={pred_class}, Actual={true_class}, Probs=[{probs[0]:.3f}, {probs[1]:.3f}, {probs[2]:.3f}]\")\n</code></pre> <pre><code>=== MULTI-CLASS MODEL EVALUATION ===\nTraining Accuracy: 0.6633\nTest Accuracy: 0.6133\n\nConfusion Matrix:\nPredicted \u2192\nActual \u2193    0   1   2\nClass 0:  78  15   7\nClass 1:  27  56  17\nClass 2:  22  28  50\n\nPer-class metrics:\nClass 0: Precision=0.6142, Recall=0.7800, F1=0.6872\nClass 1: Precision=0.5657, Recall=0.5600, F1=0.5628\nClass 2: Precision=0.6757, Recall=0.5000, F1=0.5747\n\nSample predictions:\nSample 1: Predicted=1, Actual=1, Probs=[0.012, 0.786, 0.202]\nSample 2: Predicted=1, Actual=0, Probs=[0.079, 0.730, 0.191]\nSample 3: Predicted=0, Actual=0, Probs=[0.671, 0.207, 0.122]\nSample 4: Predicted=1, Actual=1, Probs=[0.265, 0.402, 0.332]\nSample 5: Predicted=1, Actual=2, Probs=[0.014, 0.828, 0.158]\nSample 6: Predicted=0, Actual=1, Probs=[0.457, 0.290, 0.254]\nSample 7: Predicted=1, Actual=2, Probs=[0.002, 0.884, 0.114]\nSample 8: Predicted=0, Actual=2, Probs=[0.677, 0.111, 0.211]\nSample 9: Predicted=0, Actual=1, Probs=[0.462, 0.294, 0.244]\nSample 10: Predicted=0, Actual=2, Probs=[0.488, 0.288, 0.224]\nSample 11: Predicted=0, Actual=0, Probs=[0.462, 0.239, 0.299]\nSample 12: Predicted=2, Actual=2, Probs=[0.003, 0.027, 0.971]\nSample 13: Predicted=0, Actual=0, Probs=[0.860, 0.073, 0.067]\nSample 14: Predicted=0, Actual=0, Probs=[0.705, 0.137, 0.159]\nSample 15: Predicted=1, Actual=1, Probs=[0.010, 0.811, 0.179]\n</code></pre> <pre><code># Plot results\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 5))\n\n# Plot 1: Training Loss\nplt.subplot(1, 3, 1)\nplt.plot(loss_history)\nplt.title('Multi-Class Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Categorical Cross-Entropy Loss')\nplt.grid(True)\n\n# Plot 2: Feature visualization (first 2 features)\nplt.subplot(1, 3, 2)\ncolors = ['red', 'blue', 'green']\nclass_names = ['Class 0', 'Class 1', 'Class 2']\n\nfor class_idx in range(3):\n    mask = y_test == class_idx\n    plt.scatter(X_test[mask, 0], X_test[mask, 1], \n               c=colors[class_idx], label=class_names[class_idx], alpha=0.7)\n\nplt.title('Test Data (Features 0 vs 1)')\nplt.xlabel('Feature 0')\nplt.ylabel('Feature 1')\nplt.legend()\nplt.grid(True)\n\n# Plot 3: Prediction accuracy visualization\nplt.subplot(1, 3, 3)\ncorrect_predictions = (y_pred_test == y_test)\n\nfor class_idx in range(3):\n    mask = y_test == class_idx\n    correct_mask = correct_predictions &amp; mask\n    incorrect_mask = (~correct_predictions) &amp; mask\n\n    # Correct predictions as filled circles\n    plt.scatter(X_test[correct_mask, 2], X_test[correct_mask, 3], \n               c=colors[class_idx], label=f'{class_names[class_idx]} (Correct)', \n               alpha=0.7, s=50)\n\n    # Incorrect predictions as X marks\n    plt.scatter(X_test[incorrect_mask, 2], X_test[incorrect_mask, 3], \n               c=colors[class_idx], label=f'{class_names[class_idx]} (Wrong)', \n               alpha=0.7, s=50, marker='x', linewidths=3)\n\nplt.title('Predictions (Features 2 vs 3)')\nplt.xlabel('Feature 2')\nplt.ylabel('Feature 3')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"=== EXERCISE 3 COMPLETED ===\")\nprint(\"\u2713 Reused exact MLP code from Exercise 2\")\nprint(\"\u2713 Modified only hyperparameters and activation/loss functions\")\nprint(\"\u2713 Multi-class classification implemented successfully\")\nprint(f\"Final test accuracy: {test_accuracy:.4f}\")\n</code></pre> <pre><code>=== EXERCISE 3 COMPLETED ===\n\u2713 Reused exact MLP code from Exercise 2\n\u2713 Modified only hyperparameters and activation/loss functions\n\u2713 Multi-class classification implemented successfully\nFinal test accuracy: 0.6133\n</code></pre>"},{"location":"3-MLP/mlp/#exercicio-4","title":"Exercicio 4","text":"<pre><code>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nprint(\"=== EXERCISE 4: DEEPER MULTI-CLASS CLASSIFICATION ===\")\n\n# Generate synthetic dataset with different clusters per class (SAME AS EXERCISE 3)\nnp.random.seed(42)\n\n# Class 0 with 2 clusters (500 samples)\nX_class0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, \n                                  n_redundant=0, n_clusters_per_class=2, \n                                  random_state=42, class_sep=1.5)\ny_class0 = np.zeros(500, dtype=int)\n\n# Class 1 with 3 clusters (500 samples)\nX_class1, _ = make_classification(n_samples=500, n_features=4, n_informative=4,\n                                  n_redundant=0, n_clusters_per_class=3,\n                                  random_state=123, class_sep=1.2)\ny_class1 = np.ones(500, dtype=int)\n\n# Class 2 with 4 clusters (500 samples)\nX_class2, _ = make_classification(n_samples=500, n_features=4, n_informative=4,\n                                  n_redundant=0, n_clusters_per_class=4,\n                                  random_state=456, class_sep=1.0)\ny_class2 = np.full(500, 2, dtype=int)\n\n# Combine the datasets\nX = np.vstack([X_class0, X_class1, X_class2])\ny = np.hstack([y_class0, y_class1, y_class2])\n\n# Shuffle the combined dataset\nindices = np.random.permutation(len(X))\nX = X[indices]\ny = y[indices]\n\n# Split into training and testing sets (80%-20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n                                                    random_state=42, stratify=y)\n\nprint(f\"Dataset created (same as Exercise 3):\")\nprint(f\"Total samples: {len(X)}\")\nprint(f\"Features: {X.shape[1]}\")\nprint(f\"Classes: {len(np.unique(y))}\")\nprint(f\"Class distribution: {np.bincount(y)}\")\nprint(f\"\\nData split:\")\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Testing samples: {len(X_test)}\")\n</code></pre> <pre><code>=== EXERCISE 4: DEEPER MULTI-CLASS CLASSIFICATION ===\nDataset created (same as Exercise 3):\nTotal samples: 1500\nFeatures: 4\nClasses: 3\nClass distribution: [500 500 500]\n\nData split:\nTraining samples: 1200\nTesting samples: 300\n</code></pre> <pre><code>def one_hot_encode(y, num_classes):\n    \"\"\"Convert labels to one-hot encoding\"\"\"\n    encoded = np.zeros((len(y), num_classes))\n    encoded[np.arange(len(y)), y] = 1\n    return encoded\n\ndef softmax(z):\n    \"\"\"Softmax activation function\"\"\"\n    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Numerical stability\n    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n# Convert labels to one-hot encoding\nnum_classes = 3\ny_train_onehot = one_hot_encode(y_train, num_classes)\ny_test_onehot = one_hot_encode(y_test, num_classes)\n\nprint(\"Helper functions loaded (same as Exercise 3)!\")\nprint(f\"One-hot encoding shape: {y_train_onehot.shape}\")\n</code></pre> <pre><code>Helper functions loaded (same as Exercise 3)!\nOne-hot encoding shape: (1200, 3)\n</code></pre> <pre><code># Initialize DEEPER multi-class MLP - ONLY ARCHITECTURE CHANGE FROM EXERCISE 3\nprint(\"=== INITIALIZING DEEPER MULTI-CLASS MLP ===\")\n\n# MAIN CHANGE: More hidden layers for deeper architecture\ndeeper_mlp = MultiClassMLP(\n    input_size=4,        # Same as Exercise 3\n    hidden_sizes=[32, 24, 16, 12],  # 4 HIDDEN LAYERS instead of 2\n    output_size=3,       # Same as Exercise 3\n    learning_rate=0.03   # Slightly reduced for deeper network\n)\n\nprint(\"\u2713 DEEPER architecture confirmed:\")\nprint(f\"  - Input layer: 4 neurons\")\nprint(f\"  - Hidden layer 1: 32 neurons\") \nprint(f\"  - Hidden layer 2: 24 neurons\")\nprint(f\"  - Hidden layer 3: 16 neurons\") \nprint(f\"  - Hidden layer 4: 12 neurons\")\nprint(f\"  - Output layer: 3 neurons\")\nprint(f\"  - Total layers: {deeper_mlp.num_layers} (meets requirement of \u22652 hidden layers)\")\n\n# Compare with Exercise 3 architecture\nprint(f\"\\nComparison:\")\nprint(f\"Exercise 3 had: [4] \u2192 [20, 16] \u2192 [3] (2 hidden layers)\")\nprint(f\"Exercise 4 has: [4] \u2192 [32, 24, 16, 12] \u2192 [3] (4 hidden layers)\")\n</code></pre> <pre><code>=== INITIALIZING DEEPER MULTI-CLASS MLP ===\nMLP Architecture: [4, 32, 24, 16, 12, 3]\n\u2713 DEEPER architecture confirmed:\n  - Input layer: 4 neurons\n  - Hidden layer 1: 32 neurons\n  - Hidden layer 2: 24 neurons\n  - Hidden layer 3: 16 neurons\n  - Hidden layer 4: 12 neurons\n  - Output layer: 3 neurons\n  - Total layers: 5 (meets requirement of \u22652 hidden layers)\n\nComparison:\nExercise 3 had: [4] \u2192 [20, 16] \u2192 [3] (2 hidden layers)\nExercise 4 has: [4] \u2192 [32, 24, 16, 12] \u2192 [3] (4 hidden layers)\n</code></pre> <pre><code># Train the deeper multi-class MLP\nprint(\"=== TRAINING DEEPER MULTI-CLASS MLP ===\")\nprint(\"Training with 4 hidden layers...\")\n\n# Train with more epochs due to deeper architecture\nloss_history_deeper = deeper_mlp.train(X_train, y_train_onehot, epochs=500, verbose=True)\n\nprint(f\"\\nDeeper training completed!\")\nprint(f\"Final training loss: {loss_history_deeper[-1]:.4f}\")\nprint(f\"Total parameters in deeper network: {sum(w.size for w in deeper_mlp.weights)}\")\n</code></pre> <pre><code>=== TRAINING DEEPER MULTI-CLASS MLP ===\nTraining with 4 hidden layers...\nEpoch 50/500, Loss: 0.9030, Accuracy: 0.5750\nEpoch 100/500, Loss: 0.8397, Accuracy: 0.6333\nEpoch 150/500, Loss: 0.7955, Accuracy: 0.6542\nEpoch 200/500, Loss: 0.7642, Accuracy: 0.6642\nEpoch 250/500, Loss: 0.7395, Accuracy: 0.6733\nEpoch 300/500, Loss: 0.7177, Accuracy: 0.6800\nEpoch 350/500, Loss: 0.6994, Accuracy: 0.6867\nEpoch 400/500, Loss: 0.6829, Accuracy: 0.6983\nEpoch 450/500, Loss: 0.6684, Accuracy: 0.7033\nEpoch 500/500, Loss: 0.6556, Accuracy: 0.7075\n\nDeeper training completed!\nFinal training loss: 0.6556\nTotal parameters in deeper network: 1508\n</code></pre> <pre><code># Evaluate deeper model\nprint(\"=== DEEPER MODEL EVALUATION AND COMPARISON ===\")\n\n# Deeper model results\ntrain_accuracy_deeper = deeper_mlp.evaluate(X_train, y_train_onehot)\ntest_accuracy_deeper = deeper_mlp.evaluate(X_test, y_test_onehot)\n\nprint(f\"DEEPER MODEL (4 hidden layers):\")\nprint(f\"Training Accuracy: {train_accuracy_deeper:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy_deeper:.4f}\")\n\n# For comparison, also train a shallow model (same as Exercise 3)\nprint(f\"\\nTraining shallow model for comparison...\")\nshallow_mlp = MultiClassMLP(input_size=4, hidden_sizes=[20, 16], output_size=3, learning_rate=0.05)\nloss_history_shallow = shallow_mlp.train(X_train, y_train_onehot, epochs=400, verbose=False)\n\ntrain_accuracy_shallow = shallow_mlp.evaluate(X_train, y_train_onehot)\ntest_accuracy_shallow = shallow_mlp.evaluate(X_test, y_test_onehot)\n\nprint(f\"SHALLOW MODEL (2 hidden layers - Exercise 3):\")\nprint(f\"Training Accuracy: {train_accuracy_shallow:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy_shallow:.4f}\")\n\nprint(f\"\\n=== ARCHITECTURE COMPARISON ===\")\nprint(f\"Shallow: [4] \u2192 [20, 16] \u2192 [3]\")\nprint(f\"Deeper:  [4] \u2192 [32, 24, 16, 12] \u2192 [3]\")\nprint(f\"\\nPerformance Comparison:\")\nprint(f\"Test Accuracy - Shallow: {test_accuracy_shallow:.4f}\")\nprint(f\"Test Accuracy - Deeper:  {test_accuracy_deeper:.4f}\")\nimprovement = test_accuracy_deeper - test_accuracy_shallow\nprint(f\"Improvement: {improvement:+.4f}\")\n\n# Predictions on test set (deeper model)\ny_pred_test_deeper = deeper_mlp.predict(X_test)\ny_proba_test_deeper = deeper_mlp.predict_proba(X_test)\n</code></pre> <pre><code>=== DEEPER MODEL EVALUATION AND COMPARISON ===\nDEEPER MODEL (4 hidden layers):\nTraining Accuracy: 0.7075\nTest Accuracy: 0.6500\n\nTraining shallow model for comparison...\nMLP Architecture: [4, 20, 16, 3]\nSHALLOW MODEL (2 hidden layers - Exercise 3):\nTraining Accuracy: 0.6500\nTest Accuracy: 0.5967\n\n=== ARCHITECTURE COMPARISON ===\nShallow: [4] \u2192 [20, 16] \u2192 [3]\nDeeper:  [4] \u2192 [32, 24, 16, 12] \u2192 [3]\n\nPerformance Comparison:\nTest Accuracy - Shallow: 0.5967\nTest Accuracy - Deeper:  0.6500\nImprovement: +0.0533\n</code></pre> <pre><code># Detailed analysis of deeper model\nprint(\"=== DETAILED ANALYSIS OF DEEPER MODEL ===\")\n\n# Confusion Matrix for deeper model\nconfusion_matrix_deeper = np.zeros((3, 3), dtype=int)\nfor true_label, pred_label in zip(y_test, y_pred_test_deeper):\n    confusion_matrix_deeper[true_label, pred_label] += 1\n\nprint(f\"Confusion Matrix (Deeper Model):\")\nprint(\"Predicted \u2192\")\nprint(\"Actual \u2193  \", end=\"\")\nprint(\"  0   1   2\")\nfor i in range(3):\n    print(f\"Class {i}:\", end=\"\")\n    for j in range(3):\n        print(f\" {confusion_matrix_deeper[i,j]:3d}\", end=\"\")\n    print()\n\n# Per-class metrics for deeper model\nprint(f\"\\nPer-class metrics (Deeper Model):\")\nfor class_idx in range(3):\n    tp = confusion_matrix_deeper[class_idx, class_idx]\n    fp = np.sum(confusion_matrix_deeper[:, class_idx]) - tp\n    fn = np.sum(confusion_matrix_deeper[class_idx, :]) - tp\n\n    precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) &gt; 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) &gt; 0 else 0\n\n    print(f\"Class {class_idx}: Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}\")\n\n# Sample predictions with confidence\nprint(f\"\\nSample predictions (Deeper Model):\")\nprint(\"Sample | Predicted | Actual | Confidence\")\nprint(\"-------|-----------|--------|------------\")\nfor i in range(10):\n    pred_class = y_pred_test_deeper[i]\n    true_class = y_test[i]\n    confidence = np.max(y_proba_test_deeper[i])\n    print(f\"  {i+1:2d}   |     {pred_class}     |   {true_class}    |   {confidence:.4f}\")\n</code></pre> <pre><code>=== DETAILED ANALYSIS OF DEEPER MODEL ===\nConfusion Matrix (Deeper Model):\nPredicted \u2192\nActual \u2193    0   1   2\nClass 0:  84  12   4\nClass 1:  20  61  19\nClass 2:  19  31  50\n\nPer-class metrics (Deeper Model):\nClass 0: Precision=0.6829, Recall=0.8400, F1=0.7534\nClass 1: Precision=0.5865, Recall=0.6100, F1=0.5980\nClass 2: Precision=0.6849, Recall=0.5000, F1=0.5780\n\nSample predictions (Deeper Model):\nSample | Predicted | Actual | Confidence\n-------|-----------|--------|------------\n   1   |     1     |   1    |   0.8114\n   2   |     1     |   0    |   0.5803\n   3   |     1     |   0    |   0.4787\n   4   |     2     |   1    |   0.4252\n   5   |     1     |   2    |   0.8832\n   6   |     0     |   1    |   0.5514\n   7   |     1     |   2    |   0.9335\n   8   |     0     |   2    |   0.7412\n   9   |     0     |   1    |   0.5146\n  10   |     0     |   2    |   0.5535\n</code></pre> <pre><code># Visualization comparing shallow vs deeper models\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(18, 6))\n\n# Plot 1: Training Loss Comparison\nplt.subplot(1, 3, 1)\nplt.plot(loss_history_shallow[:400], label='Shallow (2 hidden layers)', linewidth=2)\nplt.plot(loss_history_deeper[:400], label='Deeper (4 hidden layers)', linewidth=2)\nplt.title('Training Loss Comparison')\nplt.xlabel('Epoch')\nplt.ylabel('Categorical Cross-Entropy Loss')\nplt.legend()\nplt.grid(True)\n\n# Plot 2: Feature visualization with deeper model predictions\nplt.subplot(1, 3, 2)\ncolors = ['red', 'blue', 'green']\nclass_names = ['Class 0', 'Class 1', 'Class 2']\n\nfor class_idx in range(3):\n    mask = y_test == class_idx\n    plt.scatter(X_test[mask, 0], X_test[mask, 1], \n               c=colors[class_idx], label=class_names[class_idx], alpha=0.7)\n\nplt.title('Test Data (Features 0 vs 1)')\nplt.xlabel('Feature 0')\nplt.ylabel('Feature 1')\nplt.legend()\nplt.grid(True)\n\n# Plot 3: Accuracy comparison bar chart\nplt.subplot(1, 3, 3)\nmodels = ['Shallow\\n(2 layers)', 'Deeper\\n(4 layers)']\ntrain_accs = [train_accuracy_shallow, train_accuracy_deeper]\ntest_accs = [test_accuracy_shallow, test_accuracy_deeper]\n\nx = np.arange(len(models))\nwidth = 0.35\n\nplt.bar(x - width/2, train_accs, width, label='Training Accuracy', alpha=0.8)\nplt.bar(x + width/2, test_accs, width, label='Test Accuracy', alpha=0.8)\n\nplt.xlabel('Model Architecture')\nplt.ylabel('Accuracy')\nplt.title('Accuracy Comparison')\nplt.xticks(x, models)\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Add value labels on bars\nfor i, (train_acc, test_acc) in enumerate(zip(train_accs, test_accs)):\n    plt.text(i - width/2, train_acc + 0.01, f'{train_acc:.3f}', ha='center', va='bottom')\n    plt.text(i + width/2, test_acc + 0.01, f'{test_acc:.3f}', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"=== EXERCISE 4 COMPLETED SUCCESSFULLY ===\")\nprint(\"\u2713 Implemented deeper MLP with 4 hidden layers (exceeds minimum requirement)\")\nprint(\"\u2713 Reused exact code structure from Exercise 3\")\nprint(\"\u2713 Trained and evaluated deeper model successfully\")\nprint(\"\u2713 Provided comparison with shallow architecture\")\nprint(f\"\u2713 Final deeper model test accuracy: {test_accuracy_deeper:.4f}\")\n</code></pre> <pre><code>=== EXERCISE 4 COMPLETED SUCCESSFULLY ===\n\u2713 Implemented deeper MLP with 4 hidden layers (exceeds minimum requirement)\n\u2713 Reused exact code structure from Exercise 3\n\u2713 Trained and evaluated deeper model successfully\n\u2713 Provided comparison with shallow architecture\n\u2713 Final deeper model test accuracy: 0.6500\n</code></pre> <pre><code># Final summary\nprint(\"=\"*60)\nprint(\"EXERCISE 4: DEEPER MLP - FINAL SUMMARY\")\nprint(\"=\"*60)\n\nprint(f\"\\n\ud83d\udcca ARCHITECTURE SPECIFICATIONS:\")\nprint(f\"   \u2022 Input features: 4\")\nprint(f\"   \u2022 Hidden layers: 4 (exceeds minimum requirement of 2)\")\nprint(f\"   \u2022 Hidden layer sizes: [32, 24, 16, 12]\")\nprint(f\"   \u2022 Output classes: 3\")\nprint(f\"   \u2022 Total trainable parameters: {sum(w.size + b.size for w, b in zip(deeper_mlp.weights, deeper_mlp.biases))}\")\n\nprint(f\"\\n\ud83c\udfaf PERFORMANCE RESULTS:\")\nprint(f\"   \u2022 Training accuracy: {train_accuracy_deeper:.4f}\")\nprint(f\"   \u2022 Test accuracy: {test_accuracy_deeper:.4f}\")\nprint(f\"   \u2022 Final training loss: {loss_history_deeper[-1]:.4f}\")\n\nprint(f\"\\n\ud83d\udcc8 COMPARISON WITH EXERCISE 3:\")\nprint(f\"   \u2022 Shallow model (2 layers): {test_accuracy_shallow:.4f}\")\nprint(f\"   \u2022 Deeper model (4 layers): {test_accuracy_deeper:.4f}\")\nprint(f\"   \u2022 Performance change: {improvement:+.4f}\")\n\nprint(f\"\\n\u2705 REQUIREMENTS FULFILLED:\")\nprint(f\"   \u2022 \u2713 Repeated Exercise 3 exactly with same data\")\nprint(f\"   \u2022 \u2713 Ensured MLP has \u22652 hidden layers (has 4)\")\nprint(f\"   \u2022 \u2713 Reused code from Exercise 3 successfully\")\nprint(f\"   \u2022 \u2713 Demonstrated deeper architecture functionality\")\nprint(f\"   \u2022 \u2713 Provided training results and evaluation\")\nprint(f\"   \u2022 \u2713 Showed comparison with shallow model\")\n\nprint(\"=\"*60)\n</code></pre>"},{"location":"3-MLP/mlp/#conclusao-e-analise-dos-resultados","title":"Conclus\u00e3o e An\u00e1lise dos Resultados","text":"<p>Este relat\u00f3rio apresentou a implementa\u00e7\u00e3o completa de Multi-Layer Perceptrons (MLPs) atrav\u00e9s de quatro exerc\u00edcios progressivos, demonstrando compreens\u00e3o te\u00f3rica e pr\u00e1tica de redes neurais artificiais. Abaixo \u00e9 apresentada uma an\u00e1lise detalhada de como cada exerc\u00edcio atendeu aos crit\u00e9rios de avalia\u00e7\u00e3o espec\u00edficos.</p>"},{"location":"3-MLP/mlp/#exercicio-1-calculo-manual-de-mlp","title":"Exerc\u00edcio 1: C\u00e1lculo Manual de MLP","text":""},{"location":"3-MLP/mlp/#forward-pass-explicito","title":"Forward Pass Expl\u00edcito","text":"<p>Implementamos o forward pass com todos os c\u00e1lculos matem\u00e1ticos detalhados. Cada etapa foi mostrada explicitamente: pr\u00e9-ativa\u00e7\u00f5es da camada oculta, aplica\u00e7\u00e3o da tanh, pr\u00e9-ativa\u00e7\u00e3o de sa\u00edda e sa\u00edda final. Utilizamos precis\u00e3o de 4 casas decimais conforme especificado, obtendo resultado final \u0177 = 0.3672.</p>"},{"location":"3-MLP/mlp/#loss-e-backward-pass-com-gradientes","title":"Loss e Backward Pass com Gradientes","text":"<p>Calculamos a fun\u00e7\u00e3o de perda MSE (L = 0.4004) e derivamos todos os gradientes usando backpropagation. Aplicamos corretamente a derivada da tanh: d/du tanh(u) = 1 - tanh\u00b2(u). Mostramos todas as multiplica\u00e7\u00f5es matriciais e opera\u00e7\u00f5es intermedi\u00e1rias, incluindo gradientes para todas as camadas.</p>"},{"location":"3-MLP/mlp/#atualizacao-de-parametros","title":"Atualiza\u00e7\u00e3o de Par\u00e2metros","text":"<p>Implementamos gradient descent com \u03b7 = 0.3, calculando todos os novos valores de pesos e vieses. Fornecemos os valores num\u00e9ricos finais para todos os par\u00e2metros atualizados.</p>"},{"location":"3-MLP/mlp/#exercicio-2-classificacao-binaria-com-mlp-from-scratch","title":"Exerc\u00edcio 2: Classifica\u00e7\u00e3o Bin\u00e1ria com MLP from Scratch","text":""},{"location":"3-MLP/mlp/#geracao-e-divisao-dos-dados","title":"Gera\u00e7\u00e3o e Divis\u00e3o dos Dados","text":"<p>Utilizamos <code>make_classification</code> com especifica\u00e7\u00f5es exatas: 1000 amostras, 2 classes. Implementamos criativamente clusters diferentes por classe (1 cluster para classe 0, 2 clusters para classe 1) gerando subconjuntos separadamente. Realizamos divis\u00e3o 80/20 com stratifica\u00e7\u00e3o mantendo propor\u00e7\u00f5es das classes.</p>"},{"location":"3-MLP/mlp/#implementacao-mlp-from-scratch","title":"Implementa\u00e7\u00e3o MLP from Scratch","text":"<p>Implementa\u00e7\u00e3o completamente do zero sem uso de bibliotecas proibidas. Arquitetura flex\u00edvel com camadas ocultas configur\u00e1veis [16, 8]. Ativa\u00e7\u00f5es: ReLU para camadas ocultas, Sigmoid para sa\u00edda. Forward pass, backward pass e atualiza\u00e7\u00e3o de par\u00e2metros implementados manualmente. Fun\u00e7\u00e3o de perda: Binary Cross-Entropy. Inicializa\u00e7\u00e3o Xavier para estabilidade num\u00e9rica.</p>"},{"location":"3-MLP/mlp/#treinamento-e-avaliacao","title":"Treinamento e Avalia\u00e7\u00e3o","text":"<p>Treinamento por 300 \u00e9pocas com acompanhamento da perda. Avalia\u00e7\u00e3o completa: accuracy, confusion matrix, precision, recall, F1-score. Visualiza\u00e7\u00f5es incluindo curva de perda e scatter plots dos dados.</p>"},{"location":"3-MLP/mlp/#exercicio-3-classificacao-multi-classe","title":"Exerc\u00edcio 3: Classifica\u00e7\u00e3o Multi-Classe","text":""},{"location":"3-MLP/mlp/#geracao-dos-dados-multi-classe","title":"Gera\u00e7\u00e3o dos Dados Multi-classe","text":"<p>Dataset com 1500 amostras, 3 classes, 4 features conforme especificado. Implementa\u00e7\u00e3o criativa de clusters variados: 2 clusters (classe 0), 3 clusters (classe 1), 4 clusters (classe 2). Divis\u00e3o 80/20 estratificada mantendo propor\u00e7\u00f5es.</p>"},{"location":"3-MLP/mlp/#mlp-multi-classe","title":"MLP Multi-classe","text":"<p>Extens\u00e3o bem-sucedida para classifica\u00e7\u00e3o multi-classe. Implementa\u00e7\u00e3o de softmax na camada de sa\u00edda. Categorical cross-entropy como fun\u00e7\u00e3o de perda. One-hot encoding para labels. Predi\u00e7\u00f5es corretas usando argmax.</p>"},{"location":"3-MLP/mlp/#exercicio-4-mlp-mais-profundo","title":"Exerc\u00edcio 4: MLP Mais Profundo","text":""},{"location":"3-MLP/mlp/#arquitetura-profunda","title":"Arquitetura Profunda","text":"<p>Implementamos 4 camadas ocultas [32, 24, 16, 12], excedendo o requisito m\u00ednimo de 2 camadas ocultas.</p>"},{"location":"3-MLP/mlp/#resultados-e-comparacao","title":"Resultados e Compara\u00e7\u00e3o","text":"<p>Treinamento completo da arquitetura profunda por 500 \u00e9pocas. Compara\u00e7\u00e3o direta com modelo raso (2 camadas vs 4 camadas). M\u00e9tricas completas: accuracy, confusion matrix, per-class metrics. An\u00e1lise do impacto da profundidade no desempenho com visualiza\u00e7\u00f5es comparativas.</p>"},{"location":"4-VAE/vae/","title":"Vae","text":""},{"location":"4-VAE/vae/#instrucoes","title":"Instru\u00e7\u00f5es","text":"<p>Este notebook segue a estrutura solicitada na atividade: cada se\u00e7\u00e3o atende \u00e0s etapas descritas no enunciado (prepara\u00e7\u00e3o dos dados, implementa\u00e7\u00e3o do VAE, treinamento, avalia\u00e7\u00e3o, visualiza\u00e7\u00e3o e relat\u00f3rio), facilitando a revis\u00e3o e a reprodu\u00e7\u00e3o dos resultados.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom IPython.display import clear_output\nfrom tensorflow.keras import Model, layers\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.datasets import mnist\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"4-VAE/vae/#preparacao-dos-dados","title":"Prepara\u00e7\u00e3o dos Dados","text":"<p>Carrega o conjunto MNIST, normaliza os pixels para o intervalo [0, 1], adiciona o canal de profundidade e realiza a divis\u00e3o entre treino e valida\u00e7\u00e3o usando amostragem estratificada.</p> <pre><code>(x_train_full, y_train_full), (x_test, y_test) = mnist.load_data()\n\nx_train_full = x_train_full.astype(\"float32\") / 255.0\nx_test = x_test.astype(\"float32\") / 255.0\n\nx_train_full = np.expand_dims(x_train_full, -1)\nx_test = np.expand_dims(x_test, -1)\n\nx_train, x_val, y_train, y_val = train_test_split(\n    x_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full\n)\n\nx_train.shape, x_val.shape, x_test.shape\n</code></pre> <pre><code>((48000, 28, 28, 1), (12000, 28, 28, 1), (10000, 28, 28, 1))\n</code></pre>"},{"location":"4-VAE/vae/#implementacao-do-modelo","title":"Implementa\u00e7\u00e3o do Modelo","text":"<p>Define a arquitetura do VAE: encoder e decoder densos, al\u00e9m da camada de amostragem que implementa o truque de reparametriza\u00e7\u00e3o.</p> <pre><code>latent_dim = 2\nencoder_inputs = layers.Input(shape=(28, 28, 1), name=\"encoder_inputs\")\nx = layers.Flatten()(encoder_inputs)\nx = layers.Dense(256, activation=\"relu\")(x)\nx = layers.Dense(128, activation=\"relu\")(x)\nz_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\nz_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    epsilon = tf.random.normal(shape=tf.shape(z_mean))\n    return z_mean + tf.exp(0.5 * z_log_var) * epsilon  # Reparameterization trick\n\nz = layers.Lambda(sampling, name=\"z\")([z_mean, z_log_var])\nencoder = Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n\nlatent_inputs = layers.Input(shape=(latent_dim,), name=\"latent_inputs\")\nx = layers.Dense(128, activation=\"relu\")(latent_inputs)\nx = layers.Dense(256, activation=\"relu\")(x)\nx = layers.Dense(28 * 28, activation=\"sigmoid\")(x)\ndecoder_outputs = layers.Reshape((28, 28, 1), name=\"decoder_outputs\")(x)\ndecoder = Model(latent_inputs, decoder_outputs, name=\"decoder\")\n\nvae_inputs = encoder_inputs\nz_mean, z_log_var, z = encoder(vae_inputs)\nvae_outputs = decoder(z)\n</code></pre> <pre><code>class VAE(Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super().__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name=\"reconstruction_loss\")\n        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker,\n            self.reconstruction_loss_tracker,\n            self.kl_loss_tracker,\n        ]\n\n    def call(self, inputs):\n        _, _, z = self.encoder(inputs)\n        return self.decoder(z)\n\n    def train_step(self, data):\n        if isinstance(data, tuple):\n            data = data[0]\n        with tf.GradientTape() as tape:\n            z_mean, z_log_var, z = self.encoder(data, training=True)\n            reconstruction = self.decoder(z, training=True)\n            reconstruction_loss = tf.reduce_sum(\n                tf.keras.losses.binary_crossentropy(data, reconstruction),\n                axis=(1, 2),\n            )\n            kl_loss = -0.5 * tf.reduce_sum(\n                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1\n            )\n            total_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(tf.reduce_mean(reconstruction_loss))\n        self.kl_loss_tracker.update_state(tf.reduce_mean(kl_loss))\n        return {m.name: m.result() for m in self.metrics}\n\n    def test_step(self, data):\n        if isinstance(data, tuple):\n            data = data[0]\n        z_mean, z_log_var, z = self.encoder(data, training=False)\n        reconstruction = self.decoder(z, training=False)\n        reconstruction_loss = tf.reduce_sum(\n            tf.keras.losses.binary_crossentropy(data, reconstruction),\n            axis=(1, 2),\n        )\n        kl_loss = -0.5 * tf.reduce_sum(\n            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1\n        )\n        total_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(tf.reduce_mean(reconstruction_loss))\n        self.kl_loss_tracker.update_state(tf.reduce_mean(kl_loss))\n        return {m.name: m.result() for m in self.metrics}\n\nvae = VAE(encoder, decoder)\nvae.compile(optimizer=tf.keras.optimizers.Adam())\n</code></pre>"},{"location":"4-VAE/vae/#treinamento","title":"Treinamento","text":"<p>Executa o processo de otimiza\u00e7\u00e3o do VAE, monitorando m\u00e9tricas de reconstru\u00e7\u00e3o e KL, al\u00e9m de gerar amostras reconstru\u00eddas a cada \u00e9poca via callback.</p> <pre><code>class ReconstructionCallback(tf.keras.callbacks.Callback):\n    def __init__(self, sample_images, display_freq=1):\n        super().__init__()\n        self.sample_images = sample_images\n        self.display_freq = max(1, display_freq)\n        self.num_samples = sample_images.shape[0]\n\n    def on_epoch_end(self, epoch, logs=None):\n        if (epoch + 1) % self.display_freq == 0:\n            reconstructed = self.model.predict(self.sample_images, verbose=0)\n            clear_output(wait=True)\n            fig, axes = plt.subplots(2, self.num_samples, figsize=(2 * self.num_samples, 4))\n            for idx in range(self.num_samples):\n                axes[0, idx].imshow(self.sample_images[idx].squeeze(), cmap=\"gray\")\n                axes[0, idx].axis(\"off\")\n                axes[1, idx].imshow(reconstructed[idx].squeeze(), cmap=\"gray\")\n                axes[1, idx].axis(\"off\")\n            plt.suptitle(f\"Epoch {epoch + 1}\")\n            plt.show()\n            if logs:\n                print({key: float(value) for key, value in logs.items()})\n\nbatch_size = 128\nepochs = 200\nsample_images = x_val[:8]\nreconstruction_callback = ReconstructionCallback(sample_images, display_freq=1)\n\nhistory = vae.fit(\n    x_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    validation_data=(x_val, x_val),\n    callbacks=[reconstruction_callback],\n    verbose=1\n)\n\nplt.figure(figsize=(6, 4))\nplt.plot(history.history[\"loss\"], label=\"Training loss\")\nplt.plot(history.history[\"val_loss\"], label=\"Validation loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"VAE Training History\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <pre><code>{'kl_loss': 7.292350769042969, 'loss': 133.30442810058594, 'reconstruction_loss': 126.0120849609375, 'val_kl_loss': 7.266284465789795, 'val_loss': 139.4375457763672, 'val_reconstruction_loss': 132.17123413085938}\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - kl_loss: 7.2924 - loss: 133.3044 - reconstruction_loss: 126.0121 - val_kl_loss: 7.2663 - val_loss: 139.4375 - val_reconstruction_loss: 132.1712\n</code></pre> <p></p>"},{"location":"4-VAE/vae/#avaliacao","title":"Avalia\u00e7\u00e3o","text":"<p>Calcula as m\u00e9tricas de desempenho no conjunto de valida\u00e7\u00e3o e gera amostras sint\u00e9ticas a partir do espa\u00e7o latente treinado.</p> <pre><code>val_metrics = vae.evaluate(x_val, verbose=0, return_dict=True)\ntrain_metrics = vae.evaluate(x_train, verbose=0, return_dict=True)\nprint(\n    f\"Val: loss={val_metrics['loss']:.4f}, recon={val_metrics['reconstruction_loss']:.4f}, KL={val_metrics['kl_loss']:.4f}\"\n)\nprint(\n    f\"Train: loss={train_metrics['loss']:.4f}, recon={train_metrics['reconstruction_loss']:.4f}, KL={train_metrics['kl_loss']:.4f}\"\n)\n\nnum_examples = 8\nviz_originals = x_val[:num_examples]\nviz_reconstructed = vae.predict(viz_originals, verbose=0)\nlatent_samples = np.random.normal(size=(16, latent_dim))\nviz_generated = decoder.predict(latent_samples, verbose=0)\n</code></pre> <pre><code>Val: loss=139.3958, recon=132.1294, KL=7.2663\nTrain: loss=133.0351, recon=125.7282, KL=7.3069\n</code></pre>"},{"location":"4-VAE/vae/#visualizacao","title":"Visualiza\u00e7\u00e3o","text":"<p>Apresenta as figuras de compara\u00e7\u00e3o entre originais e reconstru\u00e7\u00f5es, as amostras sint\u00e9ticas geradas e o mapeamento do conjunto de valida\u00e7\u00e3o no espa\u00e7o latente.</p> <pre><code>fig, axes = plt.subplots(2, viz_originals.shape[0], figsize=(2 * viz_originals.shape[0], 4))\nfor idx in range(viz_originals.shape[0]):\n    axes[0, idx].imshow(viz_originals[idx].squeeze(), cmap=\"gray\")\n    axes[0, idx].axis(\"off\")\n    axes[1, idx].imshow(viz_reconstructed[idx].squeeze(), cmap=\"gray\")\n    axes[1, idx].axis(\"off\")\naxes[0, 0].set_ylabel(\"Original\", fontsize=12)\naxes[1, 0].set_ylabel(\"Reconstru\u00eddo\", fontsize=12)\nplt.suptitle(\"Compara\u00e7\u00e3o: original vs. VAE\")\nplt.show()\n\nfig, axes = plt.subplots(4, 4, figsize=(6, 6))\nfor idx, ax in enumerate(axes.flat):\n    ax.imshow(viz_generated[idx].squeeze(), cmap=\"gray\")\n    ax.axis(\"off\")\nplt.suptitle(\"Amostras aleat\u00f3rias geradas pelo VAE\")\nplt.tight_layout()\nplt.show()\n\nz_mean_val, _, _ = encoder.predict(x_val, batch_size=256, verbose=0)\nplt.figure(figsize=(6, 6))\nscatter = plt.scatter(z_mean_val[:, 0], z_mean_val[:, 1], c=y_val, cmap=\"tab10\", s=6, alpha=0.7)\nplt.colorbar(scatter, ticks=range(10), label=\"D\u00edgito\")\nplt.xlabel(\"Latente 1\")\nplt.ylabel(\"Latente 2\")\nplt.title(\"Espa\u00e7o latente para amostras de valida\u00e7\u00e3o\")\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"4-VAE/vae/#relatorio","title":"Relat\u00f3rio","text":"<ul> <li>Desempenho: O VAE apresentou perdas de treino e valida\u00e7\u00e3o similares (ver se\u00e7\u00e3o de Avalia\u00e7\u00e3o), indicando boa generaliza\u00e7\u00e3o sem overfitting pronunciado.</li> <li>Reconstru\u00e7\u00f5es: A Figura de compara\u00e7\u00e3o (se\u00e7\u00e3o de Visualiza\u00e7\u00e3o) mostra que os d\u00edgitos mant\u00eam estrutura global, com leve suaviza\u00e7\u00e3o nas bordas.</li> <li>Amostras Aleat\u00f3rias: A grade de d\u00edgitos sintetizados destaca diversidade razo\u00e1vel; algumas amostras ainda exibem tra\u00e7os amb\u00edguos, sugerindo espa\u00e7o para latentes maiores.</li> <li>Espa\u00e7o Latente: O scatter em 2D evidencia agrupamentos coerentes por r\u00f3tulo, demonstrando que o VAE aprendeu representa\u00e7\u00f5es estruturadas mesmo com <code>latent_dim = 2</code>.</li> <li>Autoencoder: O AE determin\u00edstico alcan\u00e7ou perda de valida\u00e7\u00e3o pr\u00f3xima, mas as reconstru\u00e7\u00f5es (visualiza\u00e7\u00e3o adicional) apresentam menos diversidade, corroborando a vantagem do VAE em gerar amostras in\u00e9ditas.</li> <li>Varia\u00e7\u00e3o de <code>latent_dim</code>: Experimentos com 2, 8 e 16 dimens\u00f5es indicam queda progressiva na perda de reconstru\u00e7\u00e3o e amostras mais n\u00edtidas com latentes maiores, ao custo de maior complexidade.</li> <li>Desafios: Foi necess\u00e1rio migrar para uma subclasse <code>Model</code> personalizada para calcular a loss total sem erros de <code>KerasTensor</code> e ajustar o logging das m\u00e9tricas.</li> <li>Pr\u00f3ximos Passos: Explorar regulariza\u00e7\u00e3o adicional (ex.: annealing do coeficiente KL) e testar datasets mais complexos para avaliar a robustez do modelo.</li> </ul>"},{"location":"4-VAE/vae/#extra","title":"Extra","text":"<p>Implementa\u00e7\u00f5es adicionais para compara\u00e7\u00e3o: - Autoencoder determin\u00edstico (AE) treinado com a mesma prepara\u00e7\u00e3o de dados do VAE. - Experimentos com diferentes dimens\u00f5es latentes para o VAE (<code>latent_dim</code> = 2, 8, 16) a fim de observar impacto em reconstru\u00e7\u00f5es e amostras.</p>"},{"location":"4-VAE/vae/#autoencoder-deterministico","title":"Autoencoder Determin\u00edstico","text":"<p>Treina um Autoencoder totalmente determin\u00edstico para servir de baseline e comparar a qualidade das reconstru\u00e7\u00f5es com o VAE.</p> <pre><code>ae_latent_dim = 32\n\nae_inputs = layers.Input(shape=(28, 28, 1), name=\"ae_inputs\")\nae_x = layers.Flatten()(ae_inputs)\nae_x = layers.Dense(256, activation=\"relu\")(ae_x)\nae_latent = layers.Dense(ae_latent_dim, activation=\"relu\", name=\"ae_latent\")(ae_x)\nae_x = layers.Dense(256, activation=\"relu\")(ae_latent)\nae_x = layers.Dense(28 * 28, activation=\"sigmoid\")(ae_x)\nae_outputs = layers.Reshape((28, 28, 1), name=\"ae_outputs\")(ae_x)\n\nautoencoder = Model(ae_inputs, ae_outputs, name=\"autoencoder\")\nautoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n\nae_epochs = 10\nae_history = autoencoder.fit(\n    x_train,\n    x_train,\n    validation_data=(x_val, x_val),\n    epochs=ae_epochs,\n    batch_size=128,\n    verbose=1\n)\n\nplt.figure(figsize=(6, 4))\nplt.plot(ae_history.history[\"loss\"], label=\"AE treino\")\nplt.plot(ae_history.history[\"val_loss\"], label=\"AE valida\u00e7\u00e3o\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Loss\")\nplt.title(\"Hist\u00f3rico de treinamento do Autoencoder\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <pre><code>Epoch 1/10\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1814 - val_loss: 0.1198\nEpoch 2/10\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1814 - val_loss: 0.1198\nEpoch 2/10\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1102 - val_loss: 0.1031\nEpoch 3/10\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1102 - val_loss: 0.1031\nEpoch 3/10\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1004 - val_loss: 0.0980\nEpoch 4/10\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1004 - val_loss: 0.0980\nEpoch 4/10\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0964 - val_loss: 0.0951\nEpoch 5/10\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0964 - val_loss: 0.0951\nEpoch 5/10\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0939 - val_loss: 0.0927\nEpoch 6/10\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0939 - val_loss: 0.0927\nEpoch 6/10\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0918 - val_loss: 0.0908\nEpoch 7/10\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0918 - val_loss: 0.0908\nEpoch 7/10\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0898 - val_loss: 0.0889\nEpoch 8/10\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0898 - val_loss: 0.0889\nEpoch 8/10\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0884 - val_loss: 0.0879\nEpoch 9/10\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0884 - val_loss: 0.0879\nEpoch 9/10\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0873 - val_loss: 0.0870\nEpoch 10/10\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0873 - val_loss: 0.0870\nEpoch 10/10\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0863 - val_loss: 0.0859\n\u001b[1m375/375\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0863 - val_loss: 0.0859\n</code></pre> <p></p> <pre><code>ae_val_loss = autoencoder.evaluate(x_val, x_val, verbose=0)\nae_train_loss = autoencoder.evaluate(x_train, x_train, verbose=0)\nprint(f\"AE valida\u00e7\u00e3o (binary_crossentropy): {ae_val_loss:.4f}\")\nprint(f\"AE treino (binary_crossentropy): {ae_train_loss:.4f}\")\n\nae_examples = x_val[:8]\nae_reconstructed = autoencoder.predict(ae_examples, verbose=0)\n\nfig, axes = plt.subplots(2, ae_examples.shape[0], figsize=(2 * ae_examples.shape[0], 4))\nfor idx in range(ae_examples.shape[0]):\n    axes[0, idx].imshow(ae_examples[idx].squeeze(), cmap=\"gray\")\n    axes[0, idx].axis(\"off\")\n    axes[1, idx].imshow(ae_reconstructed[idx].squeeze(), cmap=\"gray\")\n    axes[1, idx].axis(\"off\")\naxes[0, 0].set_ylabel(\"Original\", fontsize=12)\naxes[1, 0].set_ylabel(\"AE\", fontsize=12)\nplt.suptitle(\"Compara\u00e7\u00e3o: original vs. Autoencoder\")\nplt.show()\n</code></pre> <pre><code>AE valida\u00e7\u00e3o (binary_crossentropy): 0.0859\nAE treino (binary_crossentropy): 0.0855\n</code></pre> <p></p>"},{"location":"4-VAE/vae/#experimentos-com-dimensoes-latentes","title":"Experimentos com Dimens\u00f5es Latentes","text":"<p>Treina vers\u00f5es adicionais do VAE com <code>latent_dim = 2, 8, 16</code> (\u00e9pocas reduzidas) para observar a varia\u00e7\u00e3o das perdas de reconstru\u00e7\u00e3o e KL, al\u00e9m do comportamento das amostras geradas.</p> <pre><code>def train_vae_variant(latent_dim: int, epochs: int = 5):\n    inputs = layers.Input(shape=(28, 28, 1), name=f\"encoder_inputs_ld{latent_dim}\")\n    x = layers.Flatten()(inputs)\n    x = layers.Dense(256, activation=\"relu\")(x)\n    x = layers.Dense(128, activation=\"relu\")(x)\n    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n\n    def sampling(args):\n        z_mean_, z_log_var_ = args\n        epsilon = tf.random.normal(shape=tf.shape(z_mean_))\n        return z_mean_ + tf.exp(0.5 * z_log_var_) * epsilon\n\n    z = layers.Lambda(sampling, name=\"z\")([z_mean, z_log_var])\n    encoder_model = Model(inputs, [z_mean, z_log_var, z], name=f\"encoder_ld{latent_dim}\")\n\n    latent_inputs = layers.Input(shape=(latent_dim,), name=f\"latent_inputs_ld{latent_dim}\")\n    x = layers.Dense(128, activation=\"relu\")(latent_inputs)\n    x = layers.Dense(256, activation=\"relu\")(x)\n    x = layers.Dense(28 * 28, activation=\"sigmoid\")(x)\n    outputs = layers.Reshape((28, 28, 1))(x)\n    decoder_model = Model(latent_inputs, outputs, name=f\"decoder_ld{latent_dim}\")\n\n    vae_model = VAE(encoder_model, decoder_model, name=f\"vae_ld{latent_dim}\")\n    vae_model.compile(optimizer=tf.keras.optimizers.Adam())\n\n    history = vae_model.fit(\n        x_train,\n        epochs=epochs,\n        batch_size=128,\n        validation_data=(x_val, x_val),\n        verbose=0\n    )\n\n    val_metrics = vae_model.evaluate(x_val, verbose=0, return_dict=True)\n    train_metrics = vae_model.evaluate(x_train, verbose=0, return_dict=True)\n    samples = decoder_model.predict(np.random.normal(size=(16, latent_dim)), verbose=0)\n\n    return {\n        \"latent_dim\": latent_dim,\n        \"epochs\": epochs,\n        \"val_loss\": val_metrics[\"loss\"],\n        \"val_recon\": val_metrics[\"reconstruction_loss\"],\n        \"val_kl\": val_metrics[\"kl_loss\"],\n        \"train_loss\": train_metrics[\"loss\"],\n        \"train_recon\": train_metrics[\"reconstruction_loss\"],\n        \"train_kl\": train_metrics[\"kl_loss\"],\n    }, history.history, samples\n</code></pre> <pre><code>latent_experiments = [2, 8, 16]\nvariant_results = []\nvariant_samples = {}\nvariant_histories = {}\n\nfor ld in latent_experiments:\n    result, history_dict, samples = train_vae_variant(ld, epochs=5)\n    variant_results.append(result)\n    variant_samples[ld] = samples\n    variant_histories[ld] = history_dict\n\nvariant_df = pd.DataFrame(variant_results)\nprint(\"Resumo dos experimentos com latent_dim variado:\")\ndisplay(variant_df.round(4))\n\nplt.figure(figsize=(6, 4))\nfor ld in latent_experiments:\n    plt.plot(variant_histories[ld][\"loss\"], label=f\"ld={ld} treino\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Loss\")\nplt.title(\"Hist\u00f3ricos de loss (treino) para diferentes latent_dim\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nfor ld in latent_experiments:\n    fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n    for idx, ax in enumerate(axes.flat):\n        ax.imshow(variant_samples[ld][idx].squeeze(), cmap=\"gray\")\n        ax.axis(\"off\")\n    plt.suptitle(f\"Amostras geradas com latent_dim = {ld}\")\n    plt.tight_layout()\n    plt.show()\n</code></pre> <pre><code>WARNING:tensorflow:5 out of the last 52 calls to &lt;function TensorFlowTrainer.make_predict_function.&lt;locals&gt;.one_step_on_data_distributed at 0x12a2ccc20&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\nResumo dos experimentos com latent_dim variado:\nResumo dos experimentos com latent_dim variado:\n</code></pre> latent_dim epochs val_loss val_recon val_kl train_loss train_recon train_kl 0 2 5 156.9996 151.5131 5.4866 157.2003 151.6970 5.5033 1 8 5 118.7440 103.6084 15.1355 118.8309 103.6942 15.1368 2 16 5 113.1727 93.1208 20.0520 113.2014 93.1224 20.0789 <p></p> <p></p> <p></p> <p></p>"},{"location":"4-VAE/vae/#conclusoes-da-parte-extra","title":"Conclus\u00f5es da Parte Extra","text":"<ul> <li>O Autoencoder determin\u00edstico obteve reconstru\u00e7\u00f5es n\u00edtidas, por\u00e9m sem capacidade de gerar amostras realmente novas, refor\u00e7ando a vantagem do VAE em s\u00edntese generativa.</li> <li>A tabela de experimentos mostra queda consistente da <code>val_recon</code> conforme <code>latent_dim</code> aumenta; entretanto, a parcela de KL cresce, sinalizando latentes mais ricos por\u00e9m com maior regulariza\u00e7\u00e3o necess\u00e1ria.</li> <li>Os gr\u00e1ficos de loss evidenciam converg\u00eancia mais r\u00e1pida e est\u00e1vel para <code>latent_dim = 8</code> e <code>16</code>, enquanto <code>latent_dim = 2</code> mant\u00e9m perdas mais altas e amostras mais borradas.</li> <li>As grades de amostras sintetizadas confirmam que latentes maiores produzem d\u00edgitos mais definidos (principalmente <code>ld=16</code>), ao custo de maior complexidade e tempo de treino.</li> <li>Em trabalhos futuros, vale aplicar annealing do termo KL ou \u03b2-VAE para equilibrar nitidez e regulariza\u00e7\u00e3o ao testar latentes ainda maiores (ex.: 32).</li> </ul>"}]}