---
title: "Neural Networks - Ex02"
layout: default
---

# Artificial Neural Networks and Deep 

## Ex02 - Non-Linearity in Higher Dimensions


```python
import numpy as np
```


```python
# Parametros da funcao:
meanA = [0,0,0,0,0]
covA = np.array([
    [1.0, 0.8, 0.1, 0.0, 0.0],
    [0.8, 1.0, 0.3, 0.0, 0.0],
    [0.1, 0.3, 1.0, 0.5, 0.0],
    [0.0, 0.0, 0.5, 1.0, 0.2],
    [0.0, 0.0, 0.0, 0.2, 1.0]
])

meanB = [1.5, 1.5, 1.5, 1.5, 1.5]
covB = np.array([
    [1.5, -0.7, 0.2, 0.0, 0.0],
    [-0.7, 1.5, 0.4, 0.0, 0.0],
    [0.2, 0.4, 1.5, 0.6, 0.0],
    [0.0, 0.0, 0.6, 1.5, 0.3],
    [0.0, 0.0, 0.0, 0.3, 1.5]
])

dataA = np.random.multivariate_normal(meanA,covA,500)
dataB = np.random.multivariate_normal(meanB,covB,500)

```


```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

X = np.vstack([dataA, dataB])
y = np.array(['A']*len(dataA) + ['B']*len(dataB))

X_std = StandardScaler().fit_transform(X)

pca = PCA(n_components=2)
Z = pca.fit_transform(X_std)

```


```python
# ---- a partir do seu cÃ³digo (dataA, dataB jÃ¡ criados) ----
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# 1) empilha as duas classes e cria os rÃ³tulos
X = np.vstack([dataA, dataB])            # shape (1000, 5)
y = np.array(['A']*len(dataA) + ['B']*len(dataB))

# 2) padroniza (mÃ©dia 0, desvio 1) antes do PCA
X_std = StandardScaler().fit_transform(X)

# 3) PCA para 2 componentes
pca = PCA(n_components=2)
Z = pca.fit_transform(X_std)             # shape (1000, 2)

# opcional: quanta variÃ¢ncia PC1+PC2 explicam?
print("Explained variance ratio (PC1, PC2):", pca.explained_variance_ratio_)
print("Total 2D variance explained:", pca.explained_variance_ratio_.sum())

# 4) scatter 2D colorindo por classe
plt.figure(figsize=(6,4))
for lab, col in zip(('A','B'), ('tab:blue','tab:orange')):
    idx = (y == lab)
    plt.scatter(Z[idx, 0], Z[idx, 1], s=18, label=f'Class {lab}')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.tight_layout()
plt.show()

```

    Explained variance ratio (PC1, PC2): [0.52017441 0.15376346]
    Total 2D variance explained: 0.6739378707782059



    
![png](ex02_files/ex02_4_1.png)
    



```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA, KernelPCA
from sklearn.manifold import Isomap, LocallyLinearEmbedding, SpectralEmbedding, TSNE
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import silhouette_score

try:
    from sklearn.manifold import trustworthiness
    HAS_TRUST = True
except Exception:
    HAS_TRUST = False
    def trustworthiness(*args, **kwargs):
        return np.nan

X = np.vstack([dataA, dataB])                 # (1000, 5)
y = np.array(['A']*len(dataA) + ['B']*len(dataB))  # (1000,)

# PadronizaÃ§Ã£o (mÃ©dia 0, desvio 1)
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

# -----------------------------
# 2) Definir mÃ©todos (6 no total)
# -----------------------------
methods = {}

# 1) PCA (baseline linear)
methods['PCA'] = PCA(n_components=2)

# 2) Kernel PCA (RBF) para capturar nÃ£o-linearidade
methods['KernelPCA (RBF)'] = KernelPCA(
    n_components=2, kernel='rbf', gamma=0.5, fit_inverse_transform=False
)

# 3) Isomap (distÃ¢ncias geodÃ©sicas no grafo de vizinhos)
methods['Isomap'] = Isomap(n_components=2, n_neighbors=15)

# 4) LLE (Local Linear Embedding)
methods['LLE'] = LocallyLinearEmbedding(
    n_neighbors=15, n_components=2, method='standard'
)

# 5) Spectral Embedding (Laplacian Eigenmaps)
methods['Spectral Embedding'] = SpectralEmbedding(
    n_components=2, n_neighbors=15, random_state=42
)

# 6) t-SNE (boa separaÃ§Ã£o local para visualizaÃ§Ã£o)
methods['t-SNE'] = TSNE(
    n_components=2, perplexity=30, learning_rate='auto', init='pca', random_state=42
)

# -----------------------------
# 3) FunÃ§Ãµes auxiliares
# -----------------------------
def run_embedding(model, X_input):
    """Aplica fit_transform quando existir; senÃ£o fit + transform."""
    if hasattr(model, 'fit_transform'):
        Z = model.fit_transform(X_input)
    else:
        model.fit(X_input)
        Z = model.transform(X_input)
    return Z

def plot_embedding(Z, title, y):
    """Scatter 2D com as duas classes (sem definir cores explicitamente)."""
    plt.figure(figsize=(6, 4))
    idxA = (y == 'A')
    idxB = ~idxA
    plt.scatter(Z[idxA, 0], Z[idxA, 1], s=12, label='Class A')
    plt.scatter(Z[idxB, 0], Z[idxB, 1], s=12, label='Class B')
    plt.title(title)
    plt.xlabel('Component 1')
    plt.ylabel('Component 2')
    plt.legend()
    plt.tight_layout()
    plt.show()

def fisher_separation(Z, y):
    """
    Medida simples tipo Fisher para 2 classes:
    ||muA - muB||^2 / trace(Sw),
    onde Sw = cov(ZA) + cov(ZB)
    """
    ZA = Z[y == 'A']
    ZB = Z[y == 'B']
    muA = ZA.mean(axis=0)
    muB = ZB.mean(axis=0)
    Sw = np.cov(ZA, rowvar=False) + np.cov(ZB, rowvar=False)
    num = np.sum((muA - muB)**2)
    den = np.trace(Sw)
    return float(num / den) if den > 0 else np.nan

def knn_cv_accuracy(Z, y, n_splits=5, n_neighbors=5):
    """AcurÃ¡cia mÃ©dia 5-fold CV de um k-NN (k=5) no embedding 2D."""
    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    clf = KNeighborsClassifier(n_neighbors=n_neighbors)
    scores = cross_val_score(clf, Z, y, cv=cv, scoring='accuracy')
    return float(scores.mean())

# -----------------------------
# 4) Rodar mÃ©todos + coletar mÃ©tricas
# -----------------------------
rows = []

if not HAS_TRUST:
    print("[aviso] sklearn.manifold.trustworthiness nÃ£o disponÃ­vel nesta versÃ£o; mÃ©tricas 'Trustworthiness' serÃ£o NaN.\n")

for name, model in methods.items():
    Z = run_embedding(model, X_std)

    # Plota (figura separada por mÃ©todo)
    plot_embedding(Z, f'{name}', y)

    # MÃ©tricas
    tw = trustworthiness(X_std, Z, n_neighbors=12)
    sil = silhouette_score(Z, y)        # separaÃ§Ã£o das classes no 2D
    acc = knn_cv_accuracy(Z, y, n_splits=5, n_neighbors=5)
    fish = fisher_separation(Z, y)

    # PCA: variÃ¢ncia explicada em 2D
    if name == 'PCA':
        pca_var2d = float(np.sum(methods['PCA'].explained_variance_ratio_))
    else:
        pca_var2d = np.nan

    rows.append({
        'Method': name,
        'Trustworthiness@12': tw,
        'Silhouette': sil,
        'kNN(5) CV Acc': acc,
        'Fisher Ratio': fish,
        'Explained Var (2D)': pca_var2d
    })

# -----------------------------
# 5) Mostrar tabela de mÃ©tricas
# -----------------------------
df_metrics = pd.DataFrame(rows)
df_metrics = df_metrics.sort_values(
    by=['kNN(5) CV Acc', 'Silhouette', 'Trustworthiness@12'],
    ascending=False
).reset_index(drop=True)

print("\n=== Dimensionality Reduction: Comparison Metrics ===")
print(df_metrics.to_string(index=False))
```


    
![png](ex02_files/ex02_5_0.png)
    



    
![png](ex02_files/ex02_5_1.png)
    



    
![png](ex02_files/ex02_5_2.png)
    



    
![png](ex02_files/ex02_5_3.png)
    



    
![png](ex02_files/ex02_5_4.png)
    



    
![png](ex02_files/ex02_5_5.png)
    


    
    === Dimensionality Reduction: Comparison Metrics ===
                Method  Trustworthiness@12  Silhouette  kNN(5) CV Acc  Fisher Ratio  Explained Var (2D)
                 t-SNE            0.977410    0.461134          0.920      2.826775                 NaN
    Spectral Embedding            0.851930    0.352003          0.902      1.062955                 NaN
       KernelPCA (RBF)            0.787827    0.325156          0.892      1.043269                 NaN
                   LLE            0.856462    0.274128          0.875      0.839438                 NaN
                   PCA            0.864511    0.362713          0.863      1.696896            0.673938
                Isomap            0.868698    0.382801          0.858      1.876618                 NaN


1) O que os grÃ¡ficos mostram sobre a relaÃ§Ã£o entre as classes

PCA (linear, variÃ¢ncia global)

As duas classes se deslocam principalmente ao longo do 1Âº componente (PC1).

HÃ¡ sobreposiÃ§Ã£o considerÃ¡vel na regiÃ£o central: as â€œcaudasâ€ elÃ­pticas dos dois grupos se interpenetram.

Leitura: existe um shift de mÃ©dia entre A e B (esperado, pois Âµ_B = 1.5 em todas as dimensÃµes), mas as covariÃ¢ncias diferentes (orientaÃ§Ãµes/formatos dos elipsÃ³ides) fazem os pontos â€œmisturaremâ€ quando projetados em 2D.

Kernel PCA (RBF)

Aparece um fronteira curva (formato â€œbananaâ€) entre os grupos.

O mÃ©todo estÃ¡ captando estruturas nÃ£oâ€‘lineares que o PCA linear nÃ£o mostra. A separaÃ§Ã£o melhora em relaÃ§Ã£o ao PCA, mas ainda hÃ¡ zona de transiÃ§Ã£o.

Isomap

ProjeÃ§Ã£o lembra o PCA (classes separadas grosso modo ao longo de um eixo), mas com distorÃ§Ãµes que alongam a nuvem.

Ainda existe faixa de sobreposiÃ§Ã£o: um Ãºnico hiperplano nÃ£o separa perfeitamente.

LLE

Resultado muito parecido com o PCA: tendÃªncia linear + mistura no meio.

Indica que a estrutura local nÃ£o oferece uma divisÃ£o limpa sem curvatura adicional.

Spectral Embedding

Surge uma estrutura em arco/horseshoe. As classes ocupam metades do arco, com bordas que se entremeiam.

A fronteira Ã© claramente nÃ£oâ€‘linear.

tâ€‘SNE

Dois aglomerados bem separados no plano 2D; quase nÃ£o hÃ¡ mistura visual.

Cuidado: tâ€‘SNE preserva vizinhanÃ§as locais, mas nÃ£o preserva distÃ¢ncias/forma globais. Essa separaÃ§Ã£o â€œbonitaâ€ nÃ£o garante que exista um corte linear no espaÃ§o original; ela mostra, porÃ©m, que localmente as vizinhanÃ§as de A e B sÃ£o diferentes.

Resumo da relaÃ§Ã£o entre as classes:

HÃ¡ um deslocamento de mÃ©dia entre A e B + diferenÃ§as de correlaÃ§Ã£o/orientaÃ§Ã£o (covariÃ¢ncias distintas).

Em projeÃ§Ãµes lineares (PCA/LLE/Isomap) a separaÃ§Ã£o Ã© parcial; em projeÃ§Ãµes nÃ£oâ€‘lineares (KernelPCA, Spectral, tâ€‘SNE) aparece uma borda curva e a separaÃ§Ã£o melhora.

2) As classes sÃ£o linearmente separÃ¡veis?

NÃ£o perfeitamente.

Nos mÃ©todos lineares (PCA, LLE) hÃ¡ uma faixa de interpenetraÃ§Ã£o clara.

Mesmo quando a visualizaÃ§Ã£o sugere separaÃ§Ã£o (tâ€‘SNE), isso nÃ£o implica uma hiperplano no 5D original.

Por que isso acontece aqui?

As matrizes de covariÃ¢ncia nÃ£o sÃ£o iguais: 
Î£
ğ´
â‰ 
Î£
ğµ
Î£
A
	â€‹

î€ 
=Î£
B
	â€‹

.

Para duas gaussianas com covariÃ¢ncias iguais, a fronteira Ã³tima (via razÃ£o de verossimilhanÃ§a) Ã© linear (caso LDA).

Quando as covariÃ¢ncias diferem, a fronteira Bayes Ã³tima vira quadrÃ¡tica (caso QDA) â€” nÃ£o linear.

Ã‰ exatamente o seu cenÃ¡rio: A tem correlaÃ§Ãµes fortes (e.g., 0.8 entre 
ğ‘¥
1
,
ğ‘¥
2
x
1
	â€‹

,x
2
	â€‹

, 0.5 entre 
ğ‘¥
3
,
ğ‘¥
4
x
3
	â€‹

,x
4
	â€‹

), B tem outras correlaÃ§Ãµes (e.g., â€‘0.7 entre 
ğ‘¥
1
,
ğ‘¥
2
x
1
	â€‹

,x
2
	â€‹

, 0.6 entre 
ğ‘¥
3
,
ğ‘¥
4
x
3
	â€‹

,x
4
	â€‹

). Esses â€œgiros/alongamentosâ€ elÃ­pticos diferentes fazem as nuvens se cruzarem; um hiperplano nÃ£o consegue separar sem erro.

3) Por que modelos lineares simples sofrem â€” e por que redes com nÃ£oâ€‘linearidades ajudam

Modelos lineares (Perceptron, RegressÃ£o LogÃ­stica sem features nÃ£oâ€‘lineares) sÃ³ implementam uma Ãºnica fronteira plana.

Como as classes aqui precisam de uma fronteira curvada (quadrÃ¡tica ou mais complexa) para reduzir a faixa de sobreposiÃ§Ã£o, um hiperplano vai sempre cortar a zona mista e errar bastante.

Modelos com nÃ£oâ€‘linearidade (multiâ€‘layer perceptron com ReLU/tanh, kernel methods, QDA, SVM com kernel RBF) conseguem â€œentortarâ€ o espaÃ§o ou compor curvas para aproximar a fronteira Bayesiana.

A evidÃªncia visual: Kernel PCA, Spectral e tâ€‘SNE revelam uma separaÃ§Ã£o curvada/manifoldâ€‘like, sugerindo que duas ou mais regiÃµes precisam ser recortadas por uma funÃ§Ã£o nÃ£oâ€‘linear para dividir A e B com boa acurÃ¡cia.

4) Como eu justificaria no relatÃ³rio (em 4 frases)

As projeÃ§Ãµes mostram que A e B diferem principalmente ao longo de um eixo global (PC1), mas se sobrepÃµem de forma relevante.

MÃ©todos nÃ£oâ€‘lineares (KernelPCA, Spectral, tâ€‘SNE) revelam fronteiras curvas e melhor separaÃ§Ã£o local, o que nÃ£o aparece na projeÃ§Ã£o linear.

Como 
Î£
ğ´
â‰ 
Î£
ğµ
Î£
A
	â€‹

î€ 
=Î£
B
	â€‹

, a fronteira Ã³tima entre as distribuiÃ§Ãµes gaussianas Ã© quadrÃ¡tica, logo nÃ£o linear; um hiperplano nÃ£o separa bem.

Portanto, modelos lineares tendem a ter erro alto, enquanto modelos com nÃ£oâ€‘linearidades (ex.: MLP com tanh/ReLU, SVM RBF ou QDA) sÃ£o mais adequados para capturar a geometria curvada do problema.

Se quiser, posso olhar as mÃ©tricas que vocÃª imprimiu (silhouette, kNNâ€‘acc, trustworthiness) e escrever duas linhas conectando cada mÃ©todo aos nÃºmeros que vocÃª obteve â€” isso deixa sua anÃ¡lise redonda.
