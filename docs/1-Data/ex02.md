---
title: "Neural Networks - Ex02"
layout: default
---

# Artificial Neural Networks and Deep 

## Ex02 - Non-Linearity in Higher Dimensions


```python
import numpy as np
```


```python
# Parametros da funcao:
meanA = [0,0,0,0,0]
covA = np.array([
    [1.0, 0.8, 0.1, 0.0, 0.0],
    [0.8, 1.0, 0.3, 0.0, 0.0],
    [0.1, 0.3, 1.0, 0.5, 0.0],
    [0.0, 0.0, 0.5, 1.0, 0.2],
    [0.0, 0.0, 0.0, 0.2, 1.0]
])

meanB = [1.5, 1.5, 1.5, 1.5, 1.5]
covB = np.array([
    [1.5, -0.7, 0.2, 0.0, 0.0],
    [-0.7, 1.5, 0.4, 0.0, 0.0],
    [0.2, 0.4, 1.5, 0.6, 0.0],
    [0.0, 0.0, 0.6, 1.5, 0.3],
    [0.0, 0.0, 0.0, 0.3, 1.5]
])

dataA = np.random.multivariate_normal(meanA,covA,500)
dataB = np.random.multivariate_normal(meanB,covB,500)

```


```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

X = np.vstack([dataA, dataB])
y = np.array(['A']*len(dataA) + ['B']*len(dataB))

X_std = StandardScaler().fit_transform(X)

pca = PCA(n_components=2)
Z = pca.fit_transform(X_std)

```


```python
# ---- a partir do seu código (dataA, dataB já criados) ----
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# 1) empilha as duas classes e cria os rótulos
X = np.vstack([dataA, dataB])            # shape (1000, 5)
y = np.array(['A']*len(dataA) + ['B']*len(dataB))

# 2) padroniza (média 0, desvio 1) antes do PCA
X_std = StandardScaler().fit_transform(X)

# 3) PCA para 2 componentes
pca = PCA(n_components=2)
Z = pca.fit_transform(X_std)             # shape (1000, 2)

# opcional: quanta variância PC1+PC2 explicam?
print("Explained variance ratio (PC1, PC2):", pca.explained_variance_ratio_)
print("Total 2D variance explained:", pca.explained_variance_ratio_.sum())

# 4) scatter 2D colorindo por classe
plt.figure(figsize=(6,4))
for lab, col in zip(('A','B'), ('tab:blue','tab:orange')):
    idx = (y == lab)
    plt.scatter(Z[idx, 0], Z[idx, 1], s=18, label=f'Class {lab}')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.tight_layout()
plt.show()

```

    Explained variance ratio (PC1, PC2): [0.52017441 0.15376346]
    Total 2D variance explained: 0.6739378707782059



    
![png](ex02_files/ex02_4_1.png)
    



```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA, KernelPCA
from sklearn.manifold import Isomap, LocallyLinearEmbedding, SpectralEmbedding, TSNE
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import silhouette_score

try:
    from sklearn.manifold import trustworthiness
    HAS_TRUST = True
except Exception:
    HAS_TRUST = False
    def trustworthiness(*args, **kwargs):
        return np.nan

X = np.vstack([dataA, dataB])                 # (1000, 5)
y = np.array(['A']*len(dataA) + ['B']*len(dataB))  # (1000,)

# Padronização (média 0, desvio 1)
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

# -----------------------------
# 2) Definir métodos (6 no total)
# -----------------------------
methods = {}

# 1) PCA (baseline linear)
methods['PCA'] = PCA(n_components=2)

# 2) Kernel PCA (RBF) para capturar não-linearidade
methods['KernelPCA (RBF)'] = KernelPCA(
    n_components=2, kernel='rbf', gamma=0.5, fit_inverse_transform=False
)

# 3) Isomap (distâncias geodésicas no grafo de vizinhos)
methods['Isomap'] = Isomap(n_components=2, n_neighbors=15)

# 4) LLE (Local Linear Embedding)
methods['LLE'] = LocallyLinearEmbedding(
    n_neighbors=15, n_components=2, method='standard'
)

# 5) Spectral Embedding (Laplacian Eigenmaps)
methods['Spectral Embedding'] = SpectralEmbedding(
    n_components=2, n_neighbors=15, random_state=42
)

# 6) t-SNE (boa separação local para visualização)
methods['t-SNE'] = TSNE(
    n_components=2, perplexity=30, learning_rate='auto', init='pca', random_state=42
)

# -----------------------------
# 3) Funções auxiliares
# -----------------------------
def run_embedding(model, X_input):
    """Aplica fit_transform quando existir; senão fit + transform."""
    if hasattr(model, 'fit_transform'):
        Z = model.fit_transform(X_input)
    else:
        model.fit(X_input)
        Z = model.transform(X_input)
    return Z

def plot_embedding(Z, title, y):
    """Scatter 2D com as duas classes (sem definir cores explicitamente)."""
    plt.figure(figsize=(6, 4))
    idxA = (y == 'A')
    idxB = ~idxA
    plt.scatter(Z[idxA, 0], Z[idxA, 1], s=12, label='Class A')
    plt.scatter(Z[idxB, 0], Z[idxB, 1], s=12, label='Class B')
    plt.title(title)
    plt.xlabel('Component 1')
    plt.ylabel('Component 2')
    plt.legend()
    plt.tight_layout()
    plt.show()

def fisher_separation(Z, y):
    """
    Medida simples tipo Fisher para 2 classes:
    ||muA - muB||^2 / trace(Sw),
    onde Sw = cov(ZA) + cov(ZB)
    """
    ZA = Z[y == 'A']
    ZB = Z[y == 'B']
    muA = ZA.mean(axis=0)
    muB = ZB.mean(axis=0)
    Sw = np.cov(ZA, rowvar=False) + np.cov(ZB, rowvar=False)
    num = np.sum((muA - muB)**2)
    den = np.trace(Sw)
    return float(num / den) if den > 0 else np.nan

def knn_cv_accuracy(Z, y, n_splits=5, n_neighbors=5):
    """Acurácia média 5-fold CV de um k-NN (k=5) no embedding 2D."""
    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    clf = KNeighborsClassifier(n_neighbors=n_neighbors)
    scores = cross_val_score(clf, Z, y, cv=cv, scoring='accuracy')
    return float(scores.mean())

# -----------------------------
# 4) Rodar métodos + coletar métricas
# -----------------------------
rows = []

if not HAS_TRUST:
    print("[aviso] sklearn.manifold.trustworthiness não disponível nesta versão; métricas 'Trustworthiness' serão NaN.\n")

for name, model in methods.items():
    Z = run_embedding(model, X_std)

    # Plota (figura separada por método)
    plot_embedding(Z, f'{name}', y)

    # Métricas
    tw = trustworthiness(X_std, Z, n_neighbors=12)
    sil = silhouette_score(Z, y)        # separação das classes no 2D
    acc = knn_cv_accuracy(Z, y, n_splits=5, n_neighbors=5)
    fish = fisher_separation(Z, y)

    # PCA: variância explicada em 2D
    if name == 'PCA':
        pca_var2d = float(np.sum(methods['PCA'].explained_variance_ratio_))
    else:
        pca_var2d = np.nan

    rows.append({
        'Method': name,
        'Trustworthiness@12': tw,
        'Silhouette': sil,
        'kNN(5) CV Acc': acc,
        'Fisher Ratio': fish,
        'Explained Var (2D)': pca_var2d
    })

# -----------------------------
# 5) Mostrar tabela de métricas
# -----------------------------
df_metrics = pd.DataFrame(rows)
df_metrics = df_metrics.sort_values(
    by=['kNN(5) CV Acc', 'Silhouette', 'Trustworthiness@12'],
    ascending=False
).reset_index(drop=True)

print("\n=== Dimensionality Reduction: Comparison Metrics ===")
print(df_metrics.to_string(index=False))
```


    
![png](ex02_files/ex02_5_0.png)
    



    
![png](ex02_files/ex02_5_1.png)
    



    
![png](ex02_files/ex02_5_2.png)
    



    
![png](ex02_files/ex02_5_3.png)
    



    
![png](ex02_files/ex02_5_4.png)
    



    
![png](ex02_files/ex02_5_5.png)
    


    
    === Dimensionality Reduction: Comparison Metrics ===
                Method  Trustworthiness@12  Silhouette  kNN(5) CV Acc  Fisher Ratio  Explained Var (2D)
                 t-SNE            0.977410    0.461134          0.920      2.826775                 NaN
    Spectral Embedding            0.851930    0.352003          0.902      1.062955                 NaN
       KernelPCA (RBF)            0.787827    0.325156          0.892      1.043269                 NaN
                   LLE            0.856462    0.274128          0.875      0.839438                 NaN
                   PCA            0.864511    0.362713          0.863      1.696896            0.673938
                Isomap            0.868698    0.382801          0.858      1.876618                 NaN


# Relatório de Análise: Técnicas de Redução de Dimensionalidade

## 1. Análise das Relações entre Classes

### 1.1 Principal Component Analysis (PCA)

A análise via PCA linear revela que as duas classes se distinguem principalmente ao longo do primeiro componente principal (PC1). Observa-se uma sobreposição considerável na região central dos dados, onde as distribuições elípticas dos dois grupos se interpenetram. Este comportamento indica a presença de um deslocamento de média entre as classes A e B, conforme esperado dado que µ_B = 1.5 em todas as dimensões. No entanto, as diferentes matrizes de covariância resultam em orientações e formatos elipsoidais distintos, causando mistura dos pontos quando projetados no espaço bidimensional.

### 1.2 Kernel PCA (RBF)

A aplicação do Kernel PCA com função de base radial (RBF) revela uma fronteira curva com formato característico de "banana" entre os grupos. Esta técnica demonstra sua capacidade de capturar estruturas não-lineares que não são detectáveis pelo PCA linear convencional. Embora a separação apresente melhoria em relação ao PCA linear, ainda persiste uma zona de transição entre as classes.

### 1.3 Isomap

A projeção via Isomap mantém similaridades com o PCA linear, apresentando classes separadas ao longo de um eixo principal, porém com distorções que resultam no alongamento da nuvem de pontos. A técnica preserva a faixa de sobreposição característica, confirmando que um único hiperplano não é suficiente para separação perfeita dos dados.

### 1.4 Locally Linear Embedding (LLE)

O LLE apresenta resultados consistentes com o PCA linear, mantendo a tendência de separação linear com mistura na região central. Este comportamento sugere que a estrutura local dos dados não oferece informações adicionais para uma divisão mais eficiente sem a introdução de curvatura adicional.

### 1.5 Spectral Embedding

A técnica de Spectral Embedding produz uma estrutura em formato de arco (horseshoe), onde as classes ocupam diferentes metades do arco com bordas que se entremeiam. Esta visualização evidencia claramente a natureza não-linear da fronteira de separação ótima.

### 1.6 t-distributed Stochastic Neighbor Embedding (t-SNE)

O t-SNE gera dois aglomerados bem definidos e separados no plano bidimensional, com mínima sobreposição visual. É importante considerar que o t-SNE prioriza a preservação de vizinhanças locais em detrimento de distâncias e formas globais. Portanto, embora a visualização sugira separação clara, isso não garante a existência de um corte linear no espaço original pentadimensional.

## 2. Separabilidade Linear das Classes

### 2.1 Análise de Separabilidade

Com base nas evidências obtidas através das diferentes técnicas de redução de dimensionalidade, conclui-se que as classes não são perfeitamente separáveis de forma linear. Os métodos lineares (PCA, LLE) demonstram consistentemente a presença de uma faixa de interpenetração entre as classes. Mesmo quando técnicas como t-SNE sugerem separação visual, isso não implica necessariamente a existência de um hiperplano separador no espaço original.

### 2.2 Fundamentação Teórica

A impossibilidade de separação linear perfeita decorre das diferenças entre as matrizes de covariância das classes (Σ_A ≠ Σ_B). Quando duas distribuições gaussianas possuem matrizes de covariância idênticas, a fronteira ótima baseada na razão de verossimilhança é linear (análogo ao Linear Discriminant Analysis). Entretanto, quando as covariâncias diferem, a fronteira Bayesiana ótima torna-se quadrática, caracterizando um problema de Quadratic Discriminant Analysis (QDA).

No presente estudo, a classe A apresenta correlações específicas (0.8 entre x₁ e x₂, 0.5 entre x₃ e x₄), enquanto a classe B possui correlações distintas (-0.7 entre x₁ e x₂, 0.6 entre x₃ e x₄). Essas diferenças nas orientações e alongamentos elípticos resultam no cruzamento das nuvens de pontos, impossibilitando a separação sem erro através de um hiperplano.

## 3. Limitações de Modelos Lineares e Benefícios de Não-linearidades

### 3.1 Limitações dos Modelos Lineares

Modelos lineares como Perceptron e Regressão Logística (sem features não-lineares) implementam exclusivamente fronteiras de decisão planares. Dado que as classes no presente estudo requerem fronteiras curvas (quadráticas ou de maior complexidade) para minimizar a sobreposição, um hiperplano necessariamente intersectará a zona de mistura, resultando em erros de classificação significativos.

### 3.2 Vantagens dos Modelos Não-lineares

Modelos com capacidade de não-linearidade, incluindo redes neurais multi-camada com funções de ativação ReLU/tanh, métodos kernel (SVM com kernel RBF), e QDA, possuem a habilidade de "curvar" o espaço de características ou compor funções complexas para aproximar a fronteira Bayesiana ótima.

As evidências visuais obtidas através de Kernel PCA, Spectral Embedding e t-SNE corroboram a presença de separação curva e estruturas tipo manifold, indicando que múltiplas regiões devem ser delimitadas por funções não-lineares para alcançar classificação eficaz entre as classes A e B.

## 4. Conclusões e Recomendações

As análises de redução de dimensionalidade revelam que as classes A e B apresentam diferenciação primária ao longo do primeiro componente principal, porém com sobreposição substancial nas regiões centrais. Métodos não-lineares evidenciam fronteiras curvas e melhor separação local, características não detectáveis através de projeção linear.

Considerando que Σ_A ≠ Σ_B, a fronteira ótima entre as distribuições gaussianas é intrinsecamente quadrática, portanto não-linear. Consequentemente, um hiperplano não pode proporcionar separação adequada dos dados.

Recomenda-se, portanto, a utilização de modelos com capacidade não-linear, tais como redes neurais multi-camada com funções de ativação ReLU/tanh, SVM com kernel RBF, ou análise discriminante quadrática (QDA), para capturar adequadamente a geometria curva característica do problema de classificação em questão.

---

## Anexos - Exercícios Práticos

- [Exercício 1: Análise Exploratória de Dados](1-Data/ex01.md)
- [Exercício 2: Redes Neurais](1-Data/ex02.md) 
- [Exercício 3: Modelagem Avançada](1-Data/ex03_again.md)